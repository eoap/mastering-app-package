{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mastering Earth Observation Application Packaging with CWL","text":"<p>This guide supports \"Mastering Earth Observation Application Packaging with CWL\" training events, where participants dive into the world of Earth Observation (EO) Application Packages and explore how to effectively package, share, and execute EO workflows using the Common Workflow Language (CWL) standard.</p> <p>This tutorial is designed for developers, scientists, and EO enthusiasts who want to enhance their skills in creating and sharing EO Application Packages. Whether participants are new to CWL or already have some experience, this event provides valuable insights and practical knowledge to boost their expertise.</p> <p>During a training event, the trainees will learn:</p> <ul> <li>The fundamentals of EO Application Packages and their role in the EO domain.</li> <li>How to leverage CWL to describe, package, and share workflows.</li> <li>Techniques for incorporating data, code, configuration files, and documentation into an EO Application Package.</li> <li>Best practices for creating portable and reproducible EO workflows.</li> <li>Hands-on exercises to reinforce your understanding and gain practical experience.</li> </ul> <p>When developers package an EO Application, they are in fact packaging their own software, written in a specific programming language, as a containerized application (or a set of containerized applications), to be described as an EO Application Package using the CWL as described in the OGC proposed best practices.</p> <p>This tutorial will guide participants through step-by-step examples, mastering the process of creating EO Application Packages using CWL with a simple EO application for water bodies detection using Normalised Difference Water Index (NDWI).</p>"},{"location":"#implementation","title":"Implementation","text":"<p>To facilitate this process, participants are guided through the tutorial using interactive Jupyter Notebooks across various execution scenarios:</p> <ol> <li>Application: Understand the Application and execute its individual steps (i.e. <code>crop</code>, <code>norm_diff</code>, <code>otsu</code>, <code>stac</code>) along with their associated Python modules.</li> <li>Containers: Build container images for each Application step with the required execution software dependencies, then run each step in its respective container.</li> <li>CWL-CommandLineTool: Prepare the CWL CommandLineTool document(s) wrapping the command line tool available in their associated container(s), and execute them with <code>cwltool</code>.</li> <li> <p>CWL-Workflow: Prepare the CWL Workflow orchestrating CWL CommandLineTool document(s) wrapping the command line tool available container(s). Three scenarios are demonstrated:</p> <ol> <li>Workflow that reads a single STAC Item as input and orchestrates the four steps <code>crop</code>, <code>norm_diff</code>, <code>otsu</code> and <code>stac</code>.</li> <li>Workflow that reads a STAC Catalog of a staged EO acquisition as input, then orchestrates the four steps. A first step to stage-in the EO data is also demonstrated.</li> <li>Workflow that reads multiple STAC Items as input and scatters them executing the Workflow in scenario 1. </li> </ol> </li> <li> <p>Kubernetes: Run the released Application Package CWL using <code>calrissian</code>, a CWL runner for kubernetes that creates a pod for each of the Workflow processing step. An additional Notebook is provided to demonstrate the use of the <code>calrissian</code> \"usage report\" feature to generate a resource consumption report of the execution. </p> </li> </ol>"},{"location":"#overview-flowchart-and-key-tools","title":"Overview Flowchart and Key Tools","text":"<p>The tutorial is structured in several parts:</p> %%{init: { 'logLevel': 'debug', 'theme': 'forest' } }%% timeline title Mastering EO Application Packaging with CWL Part 1 - Water bodies detection     : Short introduction      : Application steps Part 2 - Execution in Python environments     : Create Python environment     : Run Python script  Part 3 - Package the Application 1/2     : Create and test the containers     : Create the CWL CommandLineTool   : Run the CWL CommandLineTool with podman Part 4 - Package the Application 2/2     : CWL Workflow for Sentinel-2 Cloud Native processing     : CWL Workflow for Landsat-9 processing (includes stage-in/out)     : CWL Workflow of workflows  Part 5 - Release the Application     : Continuous Integration     : Containers published in a container registry     : Application Packages in a package registry Part 6 - Execution Scenarios     : Execution using a CWL runner (cwltool)     : Execution on kubernetes using the calrissian CWL runner Part 7 - FAIR Application Packages     : Recommendations and Best Practices <p>The tooling used during each part is listed below:</p> %%{init: { 'logLevel': 'debug', 'theme': 'forest' } }%% timeline title Tooling Part 1 - Water bodies detection     : N/A Part 2 - Execution in Python environments     : python venv     : python  Part 3 - Package the Application 1/2     : podman     : cwltool Part 4 - Package the Application 2/2     : cwltool Part 5 - Release the Application     : N/A Part 6 - Execution Scenarios     : cwltool     : calrissian Part 7 - FAIR Application Packages     : N/A"},{"location":"app/water-bodies-detection/","title":"Water bodies detection","text":""},{"location":"app/water-bodies-detection/#water-bodies-detection","title":"Water bodies detection","text":"<p>This application takes as input Copernicus Sentinel-2 or USSG Landsat-9 data and detects water bodies by applying the Otsu thresholding technique on the Normalized Difference Water Index (NDWI).</p> <p>The NDWI is calculated with: </p> \\[ NDWI = { (green - nir) \\over (green + nir) }  \\] <p>Typically, NDWI values of water bodies are larger than 0.2 and built-up features have positive values between 0 and 0.2.</p> <p>Vegetation has much smaller NDWI values, which results in distinguishing vegetation from water bodies easier. </p> <p>The NDWI values correspond to the following ranges:</p> Range Description 0,2 - 1 Water surface 0.0 - 0,2 Flooding, humidity -0,3 - 0.0 Moderate drought, non-aqueous surfaces -1 - -0.3 Drought, non-aqueous surfaces <p>To ease the determination of the water surface/non water surface, the Ostu thresholding technique is used. </p> <p>In the simplest form, the Otsu algorithm returns a single intensity threshold that separate pixels into two classes, foreground and background. This threshold is determined by minimizing intra-class intensity variance, or equivalently, by maximizing inter-class variance:</p> <p></p> <p>The application can be used in two modes:</p> <ul> <li>take a list of Sentinel-2 STAC items references, applies the crop over the area of interest for the radiometric bands green and NIR, the normalized difference, the Ostu threshold and finally creates a STAC catalog and items for the generated results.</li> </ul> <p>This scenario is depicted below:</p> graph TB subgraph Process STAC item   A[STAC Item] -- STAC Item URL --&gt; B   A[STAC Item] -- STAC Item URL --&gt; C   A[STAC Item] -- STAC Item URL --&gt; F subgraph scatter on bands   B[\"crop(green)\"];   C[\"crop(nir)\"]; end   B[\"crop(green)\"] -- crop_green.tif --&gt; D[Normalized difference];   C[\"crop(nir)\"] -- crop_nir.tif --&gt; D[Normalized difference];   D -- norm_diff.tif --&gt; E[Otsu threshold] end   E -- otsu.tif --&gt; F[Create STAC Catalog]   F -- \"catalog.json/item.json/asset otsu.tif\" --&gt; G[(storage)] <ul> <li>read staged Landsat-9 data as a STAC Catalog and a STAC item, applies the crop over the area of interest for the radiometric bands green and NIR, the normalized difference, the Ostu threshold and finaly creates a STAC catalog and items for the generated results.</li> </ul> <p>This scenario is depicted below:</p> graph TB subgraph stage-in   A[STAC Item] -- STAC Item URL --&gt; AA[Stage-in]   AA[Stage-in] -- catalog.json/item.json/assets blue, red,  nir ... --&gt; AB[(storage)] end subgraph Process STAC item   AB[(storage)] -- Staged STAC Catalog --&gt; B   AB[(storage)] -- Staged STAC Catalog --&gt; C   AB[(storage)] -- Staged STAC Catalog --&gt; F subgraph scatter on bands   B[\"crop(green)\"];   C[\"crop(nir)\"]; end   B[\"crop(green)\"] -- crop_green.tif --&gt; D[Normalized difference];   C[\"crop(nir)\"] -- crop_nir.tif --&gt; D[Normalized difference];   D -- norm_diff.tif --&gt; E[Otsu threshold] end   E -- otsu.tif --&gt; F[Create STAC Catalog]   F -- \"catalog.json/item.json/asset otsu.tif\" --&gt; G[(storage)] <p>Alice packages the application as an Application Package to include a macro workflow that reads the list of Sentinel-2 STAC items references or Landsat-9 staged data, launches a sub-workflow to detect the water bodies and creates the STAC catalog:</p> <p></p> <p>The sub-workflow applies the  <code>crop</code>, <code>Normalized difference</code>, <code>Otsu threshold</code> steps:</p> <p></p> <p>The development and test dataset is made of two Sentinel-2 acquisitions:</p> Acquisitions Mission Sentinel-2 Sentinel-2 Date 2022-05-24 2021-07-13 URL S2B_10TFK_20210713_0_L2A S2A_10TFK_20220524_0_L2A Quicklook <p>And one Landsat-9 acquisition:</p> Acquisition Date 2023-10-15 URL LC09_L2SP_042033_20231015_02_T1 Quicklook <p>Each <code>Command Line Tool</code> step such as <code>crop</code>, <code>Normalized difference</code>, <code>Otsu threshold</code> and <code>Create STAC</code> runs a simple Python script in a dedicated container.</p>"},{"location":"app/crop/app/","title":"Description","text":""},{"location":"app/crop/app/#step-purpose","title":"Step purpose","text":"<p>Purpose: to crop a particular band defined as a common band name (such as the \"green\" or \"nir\" band) from a satellite image acquired by either Sentinel-2 or Landsat-9. </p> <p>This step is highlighted below:</p> graph TB style B stroke:#f66,stroke-width:3px style C stroke:#f66,stroke-width:3px subgraph Process STAC item   A[STAC Item] == STAC Item URL ==&gt; B   A[STAC Item] == STAC Item URL ==&gt; C   A[STAC Item] -.-&gt; F subgraph scatter on bands   B[\"crop(green)\"];   C[\"crop(nir)\"]; end   B[\"crop(green)\"] == crop_green.tif ==&gt; D[Normalized difference];   C[\"crop(nir)\"] == crop_green.tif ==&gt; D[Normalized difference];   D -.-&gt; E[Otsu threshold] end   E -.-&gt; F[Create STAC Catalog]   F -.-&gt; G[(storage)]"},{"location":"app/crop/app/#code","title":"Code","text":"<p>The <code>crop.py</code> script is a command-line tool that takes as input</p> <ul> <li>a SpatioTemporal Asset Catalog (STAC) Item</li> <li>a bounding box area of interest (AOI), an EPSG code</li> <li>a common band name as input</li> </ul> <p>and then crops the specified band from the asset associated with the common band name to the specified AOI. </p> <p>It uses various Python libraries like <code>pystac</code>, <code>rasterio</code>, <code>pyproj</code>, <code>shapely</code>, and <code>loguru</code>.</p> <p>Here is an overview of the script's functionality:</p> <ul> <li> <p>It defines a function <code>aoi2box</code> to convert an AOI expressed as a bounding box string into a list of floats.</p> </li> <li> <p>It defines a function <code>get_asset</code> to retrieve the asset of a STAC Item that is defined with a common band name. It iterates through the assets and checks if a band has the specified common name.</p> </li> <li> <p>It defines a command-line interface using <code>click</code>, with options for providing the input STAC Item URL, AOI, EPSG code, and common band name.</p> </li> <li> <p>The <code>crop</code> function is the main entry point. It reads the STAC Item specified by the input URL and retrieves the asset associated with the common band name. It then crops the asset to the specified AOI using the rasterio library.</p> </li> <li> <p>It transforms the bounding box coordinates to match the EPSG code provided.</p> </li> <li> <p>It performs the cropping using the <code>rasterio.mask.mask</code> function.</p> </li> <li> <p>It writes the cropped image to a GeoTIFF file with a filename like \"crop_bandname.tif.\"</p> </li> </ul> <p>The script is executable as a command-line tool as its usage is:</p> <pre><code>Usage: app.py [OPTIONS]\n\n  Crops a STAC Item asset defined with its common band name\n\nOptions:\n  --input-item TEXT  STAC Item URL or staged STAC catalog  [required]\n  --aoi TEXT         Area of interest expressed as a bounding box  [required]\n  --epsg TEXT        EPSG code  [required]\n  --band TEXT        Common band name  [required]\n  --help             Show this message and exit.\n</code></pre> <p>To use this script, you would typically run it from the command line, providing the necessary input options such as the STAC Item URL, AOI, EPSG code, and common band name: </p> <pre><code>python app.py \\\n  --input-item \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_10TFK_20210713_0_L2A\" \\\n  --aoi \"-121.399,39.834,-120.74,40.472\" \\\n  --epsg \"EPSG:4326\" \\\n  --band \"green\" \n</code></pre> <p>It will then crop the specified band from the STAC asset and save it as a GeoTIFF file.</p> <p>The Python code is provided here:</p> water-bodies/command-line-tools/crop/app.py<pre><code>import os\nimport click\nimport pystac\nimport rasterio\nfrom rasterio.mask import mask\nfrom pyproj import Transformer\nfrom shapely import box\nfrom loguru import logger\n\n\ndef aoi2box(aoi):\n    \"\"\"Converts an area of interest expressed as a bounding box to a list of floats\"\"\"\n    return [float(c) for c in aoi.split(\",\")]\n\n\ndef get_asset(item, common_name):\n    \"\"\"Returns the asset of a STAC Item defined with its common band name\"\"\"\n    for _, asset in item.get_assets().items():\n        if not \"data\" in asset.to_dict()[\"roles\"]:\n            continue\n\n        eo_asset = pystac.extensions.eo.AssetEOExtension(asset)\n        if not eo_asset.bands:\n            continue\n        for b in eo_asset.bands:\n            if (\n                \"common_name\" in b.properties.keys()\n                and b.properties[\"common_name\"] == common_name\n            ):\n                return asset\n\n\n@click.command(\n    short_help=\"Crop\",\n    help=\"Crops a STAC Item asset defined with its common band name\",\n)\n@click.option(\n    \"--input-item\",\n    \"item_url\",\n    help=\"STAC Item URL or staged STAC catalog\",\n    required=True,\n)\n@click.option(\n    \"--aoi\",\n    \"aoi\",\n    help=\"Area of interest expressed as a bounding box\",\n    required=True,\n)\n@click.option(\n    \"--epsg\",\n    \"epsg\",\n    help=\"EPSG code\",\n    required=True,\n)\n@click.option(\n    \"--band\",\n    \"band\",\n    help=\"Common band name\",\n    required=True,\n)\ndef crop(item_url, aoi, band, epsg):\n\n    if os.path.isdir(item_url):\n        catalog = pystac.read_file(os.path.join(item_url, \"catalog.json\"))\n        item = next(catalog.get_items())\n    else:\n        item = pystac.read_file(item_url)\n\n    logger.info(f\"Read {item.id} from {item.get_self_href()}\")\n\n    asset = get_asset(item, band)\n    logger.info(f\"Read asset {band} from {asset.get_absolute_href()}\")\n\n    if not asset:\n        msg = f\"Common band name {band} not found in the assets\"\n        logger.error(msg)\n        raise ValueError(msg)\n\n    bbox = aoi2box(aoi)\n\n    with rasterio.open(asset.get_absolute_href()) as src:\n\n        transformer = Transformer.from_crs(epsg, src.crs, always_xy=True)\n\n        minx, miny = transformer.transform(bbox[0], bbox[1])\n        maxx, maxy = transformer.transform(bbox[2], bbox[3])\n\n        transformed_bbox = box(minx, miny, maxx, maxy)\n\n        logger.info(f\"Crop {asset.get_absolute_href()}\")\n\n        out_image, out_transform = rasterio.mask.mask(\n            src, [transformed_bbox], crop=True\n        )\n        out_meta = src.meta.copy()\n\n        out_meta.update(\n            {\n                \"height\": out_image.shape[1],\n                \"width\": out_image.shape[2],\n                \"transform\": out_transform,\n                \"dtype\": \"uint16\",\n                \"driver\": \"COG\",\n                \"tiled\": True,\n                \"compress\": \"lzw\",\n                \"blockxsize\": 256,\n                \"blockysize\": 256,\n            }\n        )\n\n        with rasterio.open(f\"crop_{band}.tif\", \"w\", **out_meta) as dst_dataset:\n            logger.info(f\"Write crop_{band}.tif\")\n            dst_dataset.write(out_image)\n\n    logger.info(\"Done!\")\n\n\nif __name__ == \"__main__\":\n    crop()\n</code></pre>"},{"location":"app/normalized-difference/app/","title":"Description","text":""},{"location":"app/normalized-difference/app/#step-purpose","title":"Step purpose","text":"<p>Purpose: to calculate the normalized difference of the \"green\" or \"nir\" bands.</p> <p>This step is highlighted below:</p> graph TB style D stroke:#f66,stroke-width:3px subgraph Process STAC item   A[STAC Item] -.-&gt; B   A[STAC Item] -.-&gt; C   A[STAC Item] -.-&gt; F subgraph scatter on bands   B[\"crop(green)\"];   C[\"crop(nir)\"]; end   B[\"crop(green)\"] == crop_green.tif ==&gt; D[Normalized difference];   C[\"crop(nir)\"] == crop_green.tif ==&gt; D[Normalized difference];   D == norm_diff.tif ==&gt; E[Otsu threshold] end   E -.-&gt; F[Create STAC Catalog]   F -.-&gt; G[(storage)]"},{"location":"app/normalized-difference/app/#code","title":"Code","text":"<p>The <code>norm_diff</code> script is a command-line tool for performing a normalized difference between two raster images. </p> <p>It uses the <code>click</code>, <code>rasterio</code>, and <code>numpy</code> libraries to perform the calculation and save the result as a GeoTIFF file. </p> <p>Here's an overview of what the script does:</p> <ul> <li> <p>It defines a command-line interface using the <code>click</code> library, with two arguments for providing the file paths of the two raster images that you want to calculate the normalized difference for.</p> </li> <li> <p>The <code>normalized_difference</code> function is the main entry point. It opens the two input raster files specified as arguments.</p> </li> <li> <p>It reads the data from the first raster (specified by <code>rasters[0]</code>) using <code>rasterio</code>, and it also copies the metadata (e.g., <code>projection</code>, <code>geotransform</code>) of this raster to be used for the output.</p> </li> <li> <p>It then opens the second raster (specified by <code>rasters[1]</code>) and reads its data.</p> </li> <li> <p>It updates the data type in the metadata to <code>\"float32\"</code> because the normalized difference result will be a floating-point image.</p> </li> <li> <p>It creates an output raster named \"norm_diff.tif\" using <code>rasterio</code>. This output raster will have the same metadata as the first input raster, but it will be of data type <code>float32</code>.</p> </li> <li> <p>It calculates the normalized difference between the two input arrays <code>(array1 - array2) / (array1 + array2)</code> and writes it to the output raster using <code>dst_dataset.write()</code>.</p> </li> </ul> <p>The script is executable as a command-line tool as its usage is:</p> <pre><code>Usage: app.py [OPTIONS] RASTERS...\n\n  Performs a normalized difference\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> <p>The Python code is provided here:</p> water-bodies/command-line-tools/norm_diff/app.py<pre><code>\"\"\"Normalized difference\"\"\"\nimport click\nimport rasterio\nimport numpy as np\nfrom loguru import logger\n\nnp.seterr(divide=\"ignore\", invalid=\"ignore\")\n\n\n@click.command(\n    short_help=\"Normalized difference\",\n    help=\"Performs a normalized difference\",\n)\n@click.argument(\"rasters\", nargs=2)\ndef normalized_difference(rasters):\n    \"\"\"Performs a normalized difference\"\"\"\n\n    logger.info(f\"Processing the normalized image with {rasters[0]} and {rasters[1]}\")\n\n    with rasterio.open(rasters[0]) as ds1:\n        array1 = ds1.read(1).astype(np.float32)\n        out_meta = ds1.meta.copy()\n\n    with rasterio.open(rasters[1]) as ds2:\n        array2 = ds2.read(1).astype(np.float32)\n\n    out_meta.update(\n        {\n            \"dtype\": \"float32\",\n            \"driver\": \"COG\",\n            \"tiled\": True,\n            \"compress\": \"lzw\",\n            \"blockxsize\": 256,\n            \"blockysize\": 256,\n        }\n    )\n\n    with rasterio.open(\"norm_diff.tif\", \"w\", **out_meta) as dst_dataset:\n        logger.info(f\"Write norm_diff.tif\")\n        dst_dataset.write((array1 - array2) / (array1 + array2), 1)\n\n    logger.info(\"Done!\")\n\n\nif __name__ == \"__main__\":\n    normalized_difference()\n</code></pre>"},{"location":"app/otsu/app/","title":"Description","text":""},{"location":"app/otsu/app/#step-purpose","title":"Step purpose","text":"<p>Purpose: to apply the Otsu threshold to the normalized difference. </p> <p>This step is highlighted below:</p> graph TB style E stroke:#f66,stroke-width:3px subgraph Process STAC item   A[STAC Item] -.-&gt; B   A[STAC Item] -.-&gt; C   A[STAC Item] -.-&gt; F subgraph scatter on bands   B[\"crop(green)\"];   C[\"crop(nir)\"]; end   B[\"crop(green)\"] -.-&gt; D[Normalized difference];   C[\"crop(nir)\"] -.-&gt; D[Normalized difference];   D == norm_diff.tif ==&gt; E[Otsu threshold] end   E == otsu.tif ==&gt; F[Create STAC Catalog]   F -.-&gt; G[(storage)]"},{"location":"app/otsu/app/#code","title":"Code","text":"<p>The <code>otsu</code> Python script is a command-line tool for applying the Otsu threshold to a single input raster image. </p> <p>It uses the <code>click</code>, <code>rasterio</code>, <code>numpy</code>, <code>skimage.filters</code>, and <code>loguru</code> libraries.</p> <p>Here's an overview of what the script does:</p> <ul> <li> <p>It defines a command-line interface using the <code>click</code> library, with a single argument for providing the file path of the input raster image on which you want to apply the Otsu threshold.</p> </li> <li> <p>The <code>otsu</code> function is the main entry point. It opens the input raster file specified as the argument.</p> </li> <li> <p>It reads the data from the input raster using <code>rasterio</code> and also copies the metadata (e.g., <code>projection</code>, <code>geotransform</code>) of this raster to be used for the output.</p> </li> <li> <p>It applies the Otsu threshold to the input array by calling the <code>threshold</code> function. The <code>threshold_otsu</code> function from <code>skimage.filters</code> is used to calculate the Otsu threshold. The thresholding process marks pixels as True or False based on whether they are greater than the calculated threshold.</p> </li> <li> <p>It creates an output raster named \"otsu.tif\" using <code>rasterio</code>. This output raster will have the same metadata as the input raster.</p> </li> <li> <p>It writes the binary image to the output raster using <code>dst_dataset.write()</code>.</p> </li> </ul> <p>The result, a binary image where pixel values are either True or False based on the thresholding, will be saved as \"otsu.tif\" in the same directory where the script is executed.</p> <p>The script is executable as a command-line tool as its usage is:</p> <pre><code>Usage: app.py [OPTIONS] RASTER\n\n  Applies the Otsu threshold\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> <p>The Python code is provided here:</p> water-bodies/command-line-tools/otsu/app.py<pre><code>\"\"\"Command line tool to apply the Otsu threshold to a raster\"\"\"\nimport click\nimport rasterio\nimport numpy as np\nfrom skimage.filters import threshold_otsu\nfrom loguru import logger\n\n\ndef threshold(data):\n    \"\"\"Returns the Otsu threshold of a numpy array\"\"\"\n    return data &gt; threshold_otsu(data[np.isfinite(data)])\n\n\n@click.command(\n    short_help=\"Otsu threshoold\",\n    help=\"Applies the Otsu threshold\",\n)\n@click.argument(\"raster\", nargs=1)\ndef otsu(raster):\n    \"\"\"Applies the Otsu threshold\"\"\"\n\n    with rasterio.open(raster) as ds:\n        array = ds.read(1)\n        out_meta = ds.meta.copy()\n\n    out_meta.update(\n        {\n            \"dtype\": \"uint8\",\n            \"driver\": \"COG\",\n            \"tiled\": True,\n            \"compress\": \"lzw\",\n            \"blockxsize\": 256,\n            \"blockysize\": 256,\n        }\n    )\n\n    logger.info(f\"Applying the Otsu threshold to {raster}\")\n\n    with rasterio.open(\"otsu.tif\", \"w\", **out_meta) as dst_dataset:\n        logger.info(f\"Write otsu.tif\")\n        dst_dataset.write(threshold(array), indexes=1)\n\n    logger.info(\"Done!\")\n\n\nif __name__ == \"__main__\":\n    otsu()\n</code></pre>"},{"location":"app/stac/app/","title":"Description","text":""},{"location":"app/stac/app/#step-purpose","title":"Step purpose","text":"<p>Purpose: produce a STAC Catalog with a STAC Item describing the detected water body result. </p> <p>This step is highlighted below:</p> graph TB style F stroke:#f66,stroke-width:3px subgraph Process STAC item   A[STAC Item] -.-&gt; B   A[STAC Item] -.-&gt; C   A[STAC Item] == STAC Item URL ==&gt; F subgraph scatter on bands   B[\"crop(green)\"];   C[\"crop(nir)\"]; end   B[\"crop(green)\"] -.-&gt; D[Normalized difference];   C[\"crop(nir)\"] -.-&gt; D[Normalized difference];   D -.-&gt; E[Otsu threshold] end   E == otsu.tif ==&gt; F[Create STAC Catalog]   F == \"catalog.json/item.json/asset otsu.tif\" ==&gt; G[(storage)]"},{"location":"app/stac/app/#code","title":"Code","text":"<p>The <code>stac</code> script is a command-line tool for creating a SpatioTemporal Asset Catalog (STAC) catalog containing detected water bodies.</p> <p>It takes the STAC Item URLs and corresponding water body GeoTIFFs as input, creates STAC Items for each water body, and organizes them into a STAC Catalog. </p> <p>The script utilizes the <code>click</code>, <code>pystac</code>, <code>rio_stac</code>, and <code>loguru</code> libraries. </p> <p>Here's an overview of what the script does:</p> <ul> <li> <p>It defines a command-line interface using the <code>click</code> library. The script expects multiple input STAC Item URLs and corresponding water body GeoTIFFs as arguments.</p> </li> <li> <p>The <code>to_stac</code> function is the main entry point. It creates a new STAC catalog and iterates through the provided STAC Item URLs and water body GeoTIFFs.</p> </li> <li> <p>For each input pair (STAC Item URL and GeoTIFF), it reads the STAC Item, creates a directory with the same name as the item's ID, and copies the water body GeoTIFF into that directory.</p> </li> <li> <p>It uses the <code>rio_stac</code> library to create a STAC Item for the water body. This includes specifying the source GeoTIFF, input date/time, asset roles, asset href, and more.</p> </li> <li> <p>The created STAC Items are added to the STAC catalog.</p> </li> <li> <p>After processing all input pairs, it saves the STAC catalog to the root directory, specifying it as a self-contained catalog with the catalog type set to <code>pystac.CatalogType.SELF_CONTAINED</code>.</p> </li> </ul> <p>The script will create a STAC catalog containing the detected water bodies and save it in the current directory. </p> <p>The script is executable as a command-line tool as its usage is:</p> <pre><code>Usage: app.py [OPTIONS]\n\n  Creates a STAC catalog with the water bodies\n\nOptions:\n  --input-item TEXT  STAC Item URL  [required]\n  --water-body TEXT  Water body geotiff  [required]\n  --help             Show this message and exit.\n</code></pre> <p>The Python code is provided here:</p> water-bodies/command-line-tools/stac/app.py<pre><code>\"\"\"Creates a STAC catalog with the detected water bodies\"\"\" \"\"\nimport os\nimport shutil\nimport click\nimport pystac\nimport rio_stac\nfrom loguru import logger\n\n\n@click.command(\n    short_help=\"Creates a STAC catalog\",\n    help=\"Creates a STAC catalog with the water bodies\",\n)\n@click.option(\n    \"--input-item\",\n    \"item_urls\",\n    help=\"STAC Item URL\",\n    required=True,\n    multiple=True,\n)\n@click.option(\n    \"--water-body\",\n    \"water_bodies\",\n    help=\"Water body geotiff\",\n    required=True,\n    multiple=True,\n)\ndef to_stac(item_urls, water_bodies):\n    \"\"\"Creates a STAC catalog with the detected water bodies\"\"\"\n\n    logger.info(f\"Creating a STAC Catalog for {' '.join(water_bodies)}\")\n    cat = pystac.Catalog(id=\"catalog\", description=\"water-bodies\")\n\n    for index, item_url in enumerate(item_urls):\n        if os.path.isdir(item_url):\n            catalog = pystac.read_file(os.path.join(item_url, \"catalog.json\"))\n            item = next(catalog.get_items())\n        else:\n            item = pystac.read_file(item_url)\n\n        water_body = water_bodies[index]\n\n        os.makedirs(item.id, exist_ok=True)\n        shutil.copy(water_body, item.id)\n\n        out_item = rio_stac.stac.create_stac_item(\n            source=water_body,\n            input_datetime=item.datetime,\n            id=item.id,\n            asset_roles=[\"data\", \"visual\"],\n            asset_href=os.path.basename(water_body),\n            asset_name=\"data\",\n            with_proj=True,\n            with_raster=True,\n        )\n\n        cat.add_items([out_item])\n\n    cat.normalize_and_save(\n        root_href=\"./\", catalog_type=pystac.CatalogType.SELF_CONTAINED\n    )\n    logger.info(\"Done!\")\n\n\nif __name__ == \"__main__\":\n    to_stac()\n</code></pre>"},{"location":"app-package/ogc-context/","title":"OGC Activities","text":"<p>Previous OGC Testbeds 13-16 initiated the design of an application package for Earth Observation Applications in distributed Cloud Platforms </p> <p>The application package provides information about the software item, metadata and dependencies</p> <p>The application package can be deployed and executed within an Exploitation Platform in a service compliant with the OGC API Processes specification</p> <p>OGC 20-089 defines the Best Practice to package and deploy Earth Observation Applications in an Exploitation Platform</p> <p>OGC 20-089 defines guidance for the 3 viewpoints:</p> <ul> <li>For a Developer to adapt an application </li> <li>For an Integrator to package an application </li> <li>For an Platform to deploy and execute the application</li> </ul> <p>12 submitters organisations:</p> <ul> <li>Pedro Gon\u00e7alves (editor)  Terradue</li> <li>Fabrice Brito Terradue</li> <li>Tom Landry    CRIM</li> <li>Francis Charette-Migneault    CRIM</li> <li>Richard Conway    Telespazio VEGA UK</li> <li>Adrian Luna   European Union Satellite Centre</li> <li>Omar Barrilero    European Union Satellite Centre</li> <li>Panagiotis (Peter) A. Vretanos    CubeWerx Inc.</li> <li>Cristiano Lopes   European Space Agency (ESA)</li> <li>Antonio Romeo RHEA Group</li> <li>Paulo Sacramento  Solenix</li> <li>Samantha Lavender Pixalytics</li> <li>Marian Neagul West University of Timisoara</li> </ul>"},{"location":"app-package/ogc-context/#application-package-drivers","title":"Application Package Drivers","text":"<p>Decouple application developers from exploitation platform operators and from application consumers:</p> <ul> <li>Focus on application development by minimizing platform specific particularities</li> <li>Make their applications compatible with several execution scenarios </li> </ul> <p>Enable exploitation platforms to virtually support any type of packaged EO application</p>"},{"location":"app-package/ogc-context/#application-package-role","title":"Application Package Role","text":"<p>The Application Package:</p> <ul> <li>Describes the data processing application by providing information about parameters, software item, executable, dependencies and metadata. </li> <li>Ensures that the application is fully portable supporting execution and automatic deployment in a Machine-To- Machine (M2M) scenario and execution in other scenarios. </li> <li>Contains an information model to allow its deployment of the application as OGC API - Processes compliant web service</li> </ul>"},{"location":"app-package/ogc-context/#application-package-process","title":"Application Package Process","text":"<p>Application developers:</p> <ul> <li>Create containers with their runtime environment, dependencies and command line tools</li> <li>Orchestrate the processing steps in a Directed Acyclic Graph (DAG)</li> <li>Use fan-out or fan-in patterns at step level to exploit distributed computing resources</li> <li>Describe their Application Package using the Common Workflow Language (CWL) </li> </ul>"},{"location":"app-package/ogc-context/#about-the-common-workflow-language-cwl","title":"About the Common Workflow Language (CWL)","text":"<p>The Common Workflow Language (CWL) is an open standard for describing analysis workflows and tools in a way that makes them portable and scalable across a variety of software and hardware environments, from workstations to cluster, cloud, and high performance computing environments.</p> <p>The CWL is a specification for describing analysis workflows and tools.</p> <p>CWL workflows are portable and scalable across a variety of software and hardware environments, from workstations to cluster, cloud, and high performance computing (HPC) environments.</p> <p>CWL is designed to meet the needs of data-intensive science, such as Bioinformatics, Medical Imaging, Astronomy, Physics, and Chemistry.</p>"},{"location":"benchmarking/calrissian-benchmark/","title":"Application benchmarking","text":""},{"location":"benchmarking/calrissian-benchmark/#goal","title":"Goal","text":"<p>Run the app-water-body-cloud-native.1.0.0.cwl released application package using calrissian, the CWL runner for kubernetes and exploit the usage report.</p>"},{"location":"benchmarking/calrissian-benchmark/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/Kubernetes/benchmark.ipynb</p>"},{"location":"containers/crop/","title":"Crop","text":""},{"location":"containers/crop/#goal","title":"Goal","text":"<p>Create a container and run the <code>crop</code> step in the container image tagged and built in the previous step.</p>"},{"location":"containers/crop/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/Containers/crop.ipynb</p>"},{"location":"containers/crop/#container","title":"Container","text":"<p>Each step has its own recipe to build the container image.</p> <p>The <code>crop</code> step container image recipe is:</p> crop/Dockerfile<pre><code>FROM docker.io/library/python:3.10-slim@sha256:80619a5316afae7045a3c13371b0ee670f39bac46ea1ed35081d2bf91d6c3dbd\n\nRUN pip install --no-cache-dir rasterio click pystac loguru pyproj shapely &amp;&amp; \\\n    python -c \"import rasterio\"\n\nADD app.py /app/app.py\n\nENTRYPOINT []\n</code></pre> <p>Build the container image with:</p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\n\npodman build --format docker -t localhost/crop:latest ${WORKSPACE}/water-bodies/command-line-tools/crop\n</code></pre>"},{"location":"containers/crop/#how-to-run-a-step-in-a-container","title":"How to run a step in a container","text":"<p>We'll use <code>podman</code> container engine (<code>docker</code> is also fine).</p> <p>The command to run the <code>crop</code> step in the container is:</p> <pre><code>podman run \\\n    -i \\\n    --userns=keep-id \\\n    --mount=type=bind,source=/workspace/mastering-app-package/runs,target=/runs \\\n    --workdir=/runs \\\n    --read-only=true \\\n    --user=1001:100 \\\n    --rm \\\n    --env=HOME=/runs \\\n    --env=PYTHONPATH=/app \\\n    localhost/crop:latest \\\n    python \\\n    -m \\\n    app \\\n    --aoi \\\n    \"-121.399,39.834,-120.74,40.472\" \\\n    --band \\\n    green \\\n    --epsg \\\n    \"EPSG:4326\" \\\n    --input-item \\\n    https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_10TFK_20210713_0_L2A\n</code></pre> <p>Let's break down what this command does:</p> <ul> <li><code>podman run</code>: This is the command to run a container.</li> <li><code>-i</code>: This flag makes the container interactive, allowing you to interact with it via the terminal.</li> <li><code>--userns=keep-id</code>: It instructs <code>podman</code> to keep the user namespace ID. <code>--mount=type=bind,source=/workspace/mastering-app-package/runs,target=/runs</code>: This option mounts a directory from the host system to the container. In this case, it mounts the <code>/workspace/mastering-app-package/runs</code> directory on the host to the /runs directory inside the container.</li> <li><code>--workdir=/runs</code>: Sets the working directory inside the container to <code>/runs</code>.</li> <li><code>--read-only=true</code>: Makes the file system inside the container read-only, meaning you can't write or modify files inside the container.</li> <li><code>--user=1001:100</code>: Specifies the user and group IDs to be used within the container.</li> <li><code>--rm</code>: This flag tells podman to remove the container after it has finished running.</li> <li><code>--env=HOME=/runs</code>: Sets the <code>HOME</code> environment variable inside the container to <code>/runs</code>.</li> <li><code>--env=PYTHONPATH=/app</code>: Sets the <code>PYTHONPATH</code> environment variable inside the container to <code>/app</code>.</li> <li><code>localhost/crop:latest</code>: This is the name of the container image that you want to run. It's pulling the image from the local container registry with the name \"crop\" and the \"latest\" tag.</li> <li><code>python -m app</code>: This is the command to run inside the container. It runs a Python module named \"app.\"</li> <li><code>--aoi \"-121.399,39.834,-120.74,40.472\"</code>: This provides command-line arguments to the Python module. It specifies the area of interest (AOI) as a bounding box.</li> <li><code>--band green</code>: Specifies the band to be extracted from the Sentinel-2 acquisition. In this case, it's the \"green\" band.</li> <li><code>--epsg \"EPSG:4326\"``: Specifies the EPSG code, which defines the coordinate system used for the</code>aoi` command-line argument.</li> <li><code>--input-item ...</code>: Specifies the input STAC item URL. This particular URL points to a Sentinel-2 image hosted on AWS Earth Search.</li> </ul>"},{"location":"containers/crop/#steps","title":"Steps","text":"<p>Clean-up the <code>/workspace/mastering-app-package/runs</code> folder: </p> <pre><code>rm -fr /workspace/mastering-app-package/runs/*\n</code></pre> <p>Run the command to run the <code>crop</code> step with the <code>green</code> band in the container with:</p> <pre><code>sh ${WORKSPACE}/scripts/podman-crop-green.sh\n</code></pre> <p>Run the command to run the <code>crop</code> step with the <code>nir</code> band in the container with:</p> <pre><code>sh -x ${WORKSPACE}/scripts/podman-crop-nir.sh\n</code></pre>"},{"location":"containers/crop/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-mrossi:~/runs$ tree .\n.\n\u251c\u2500\u2500 crop_green.tif\n\u2514\u2500\u2500 crop_nir.tif\n\n0 directories, 2 files\n</code></pre>"},{"location":"containers/norm-diff/","title":"Normalized difference","text":""},{"location":"containers/norm-diff/#goal","title":"Goal","text":"<p>Create a container and run the <code>norm_diff</code> step in the container image tagged <code>localhost/norm-diff:latest</code>.</p>"},{"location":"containers/norm-diff/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/Containers/norm-diff.ipynb</p>"},{"location":"containers/norm-diff/#container","title":"Container","text":"<p>Each step has its own recipe to build the container image.</p> <p>The <code>norm_diff</code> step container image recipe is:</p> norm_diff/Dockerfile<pre><code>FROM docker.io/library/python:3.10-slim@sha256:80619a5316afae7045a3c13371b0ee670f39bac46ea1ed35081d2bf91d6c3dbd\n\nRUN pip install --no-cache-dir rasterio click loguru &amp;&amp; \\\n    python -c \"import rasterio\"\n\nADD app.py /app/app.py\n\nENTRYPOINT []\n</code></pre> <p>Build the container image with:</p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\n\npodman build --format docker -t localhost/norm-diff:latest ${WORKSPACE}/water-bodies/command-line-tools/norm_diff\n</code></pre>"},{"location":"containers/norm-diff/#run-the-container","title":"Run the container","text":"<p>The command to run the <code>norm_diff</code> step in the container is:</p> <pre><code>podman \\\n    run \\\n    -i \\\n    --userns=keep-id \\\n    --mount=type=bind,source=/workspace/mastering-app-package/runs,target=/runs \\\n    --mount=type=bind,source=/workspace/mastering-app-package/runs/crop_green.tif,target=/inputs/crop_green.tif,readonly \\\n    --mount=type=bind,source=/workspace/mastering-app-package/runs/crop_nir.tif,target=/inputs/crop_nir.tif,readonly \\\n    --workdir=/runs \\\n    --read-only=true \\\n    --user=1001:100 \\\n    --rm \\\n    --env=HOME=/runs \\\n    --env=PYTHONPATH=/app \\\n    localhost/norm-diff:latest \\\n    python \\\n    -m \\\n    app \\\n    /inputs/crop_green.tif \\\n    /inputs/crop_nir.tif\n</code></pre>"},{"location":"containers/norm-diff/#step","title":"Step","text":"<p>Run the command to run the <code>norm_diff</code> step with:</p> <pre><code>sh -x ${WORKSPACE}/scripts/podman-norm-diff.sh\n</code></pre>"},{"location":"containers/norm-diff/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-mrossi:~/runs$ tree .\n.\n\u251c\u2500\u2500 crop_green.tif\n\u251c\u2500\u2500 crop_nir.tif\n\u2514\u2500\u2500 norm_diff.tif\n\n0 directories, 3 files\n</code></pre>"},{"location":"containers/otsu/","title":"Ostu threshold","text":""},{"location":"containers/otsu/#goal","title":"Goal","text":"<p>Create a container and run the <code>otsu</code> step in the container image tagged <code>localhost/otsu:latest</code>.</p>"},{"location":"containers/otsu/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/Containers/otsu.ipynb</p>"},{"location":"containers/otsu/#container","title":"Container","text":"<p>Each step has its own recipe to build the container image.</p> <p>The <code>otsu</code> step container image recipe is:</p> otsu/Dockerfile<pre><code>FROM docker.io/library/python:3.10-slim@sha256:80619a5316afae7045a3c13371b0ee670f39bac46ea1ed35081d2bf91d6c3dbd\n\nRUN pip install --no-cache-dir rasterio scikit-image click loguru &amp;&amp; \\\n    python -c \"import rasterio\"\n\nADD app.py /app/app.py\n\nENTRYPOINT []\n</code></pre> <p>Build the container image with:</p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\n\npodman build --format docker -t localhost/otsu:latest ${WORKSPACE}/water-bodies/command-line-tools/otsu\n</code></pre>"},{"location":"containers/otsu/#run-the-container","title":"Run the container","text":"<p>The command to run the <code>otsu</code> step in the container is:</p> <pre><code>podman \\\n    run \\\n    -i \\\n    --userns=keep-id \\\n    --mount=type=bind,source=/workspace/mastering-app-package/runs,target=/runs \\\n    --mount=type=bind,source=/workspace/mastering-app-package/runs/norm_diff.tif,target=/inputs/norm_diff.tif,readonly \\\n    --workdir=/runs \\\n    --read-only=true \\\n    --user=1001:100 \\\n    --rm \\\n    --env=HOME=/runs \\\n    --env=PYTHONPATH=/app \\\n    localhost/otsu:latest \\\n    python \\\n    -m \\\n    app \\\n    /inputs/norm_diff.tif\n</code></pre>"},{"location":"containers/otsu/#step","title":"Step","text":"<p>Run the command to run the <code>otsu</code> step with:</p> <pre><code>sh -x ${WORKSPACE}/scripts/podman-otsu.sh\n</code></pre>"},{"location":"containers/otsu/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-mrossi:~/runs$ tree .\n.\n\u251c\u2500\u2500 crop_green.tif\n\u251c\u2500\u2500 crop_nir.tif\n\u251c\u2500\u2500 norm_diff.tif\n\u2514\u2500\u2500 otsu.tif\n\n0 directories, 4 files\n</code></pre>"},{"location":"containers/scope/","title":"Scope","text":"<p>When developers package and EO, they are in fact packaging their own software, written in a specific programming language, as a containerized application (or a set of containerized applications), to be described as an EO Application Package using the Common Workflow Language as described in the OGC proposed best practices.</p> <p>To achieve this, developers follow the steps described below.</p> <ul> <li>Prepare one or more container images containing the execution dependencies of the software.</li> <li>Prepare the CWL CommandLineTool document(s) wrapping the command line tool available container(s).</li> <li>Prepare the CWL Workflow orchestrating CWL CommandLineTool document(s) wrapping the command line tool available container(s).</li> <li>Test the application package in one or more execution scenarios.</li> </ul> <p>This section shows how to do the step:</p> <ul> <li>Prepare one or more container images containing the execution dependencies of the software.</li> </ul>"},{"location":"containers/stac/","title":"STAC Catalog","text":""},{"location":"containers/stac/#goal","title":"Goal","text":"<p>Create a container and run the <code>stac</code> step in the container image tagged <code>localhost/stac:latest</code>.</p>"},{"location":"containers/stac/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/Containers/stac.ipynb</p>"},{"location":"containers/stac/#container","title":"Container","text":"<p>Each step has its own recipe to build the container image.</p> <p>The <code>stac</code> step container image recipe is:</p> stac/Dockerfile<pre><code>FROM docker.io/library/python:3.10-slim@sha256:80619a5316afae7045a3c13371b0ee670f39bac46ea1ed35081d2bf91d6c3dbd\n\nRUN pip install --no-cache-dir pystac rio_stac loguru click &amp;&amp; \\\n    python -c \"import rio_stac\"\n\nADD app.py /app/app.py\n\nENTRYPOINT []\n</code></pre> <p>Build the container image with:</p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\n\npodman build --format docker -t localhost/crop:latest ${WORKSPACE}/water-bodies/command-line-tools/crop\npodman build --format docker -t localhost/norm-diff:latest ${WORKSPACE}/water-bodies/command-line-tools/norm_diff\npodman build --format docker -t localhost/otsu:latest ${WORKSPACE}/water-bodies/command-line-tools/otsu\n</code></pre>"},{"location":"containers/stac/#run-the-container","title":"Run the container","text":"<p>The command to run the <code>stac</code> step in the container is:</p> <pre><code>podman \\\n    run \\\n    -i \\\n    --userns=keep-id \\\n    --mount=type=bind,source=/workspace/mastering-app-package/runs,target=/runs \\\n    --mount=type=bind,source=/workspace/mastering-app-package/runs/otsu.tif,target=/inputs/otsu.tif,readonly \\\n    --workdir=/runs \\\n    --read-only=true \\\n    --user=1001:100 \\\n    --rm \\\n    --env=HOME=/runs \\\n    --env=PYTHONPATH=/app \\\n    localhost/stac:latest \\\n    python \\\n    -m \\\n    app \\\n    --input-item \\\n    https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_10TFK_20210713_0_L2A \\\n    --water-body \\\n    /inputs/otsu.tif\n</code></pre>"},{"location":"containers/stac/#step","title":"Step","text":"<p>Run the command to run the <code>stac</code> step with:</p> <pre><code>sh -x ${WORKSPACE}/scripts/podman-stac.sh\n</code></pre>"},{"location":"containers/stac/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-mrossi:~/runs$ tree .\n.\n\u251c\u2500\u2500 S2B_10TFK_20210713_0_L2A\n\u2502   \u251c\u2500\u2500 S2B_10TFK_20210713_0_L2A.json\n\u2502   \u2514\u2500\u2500 otsu.tif\n\u251c\u2500\u2500 catalog.json\n\u251c\u2500\u2500 crop_green.tif\n\u251c\u2500\u2500 crop_nir.tif\n\u251c\u2500\u2500 norm_diff.tif\n\u2514\u2500\u2500 otsu.tif\n\n1 directory, 7 files\n</code></pre>"},{"location":"cwl-cli/crop/","title":"Crop","text":""},{"location":"cwl-cli/crop/#goal","title":"Goal","text":"<p>Wrap the <code>crop</code> step as a Common Workflow Language CommandLineTool and execute it with a CWL runner.</p>"},{"location":"cwl-cli/crop/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/CommandLineTools/crop.ipynb</p>"},{"location":"cwl-cli/crop/#how-to-wrap-a-step-as-a-cwl-commandlinetool","title":"How to wrap a step as a CWL CommandLineTool","text":"<p>The CWL document below shows the <code>crop</code> step wrapped as a CWL CommandLineTool:</p> cwl-cli/crop.cwl<pre><code>cwlVersion: v1.0\n\nclass: CommandLineTool\nid: crop\nrequirements:\n    InlineJavascriptRequirement: {}\n    EnvVarRequirement:\n      envDef:\n        PYTHONPATH: /app\n    ResourceRequirement:\n      coresMax: 1\n      ramMax: 512\nhints:\n  DockerRequirement:\n    dockerPull: localhost/crop:latest\nbaseCommand: [\"python\", \"-m\", \"app\"]\narguments: []\ninputs:\n  item:\n    type: string\n    inputBinding:\n        prefix: --input-item\n  aoi:\n    type: string\n    inputBinding:\n        prefix: --aoi\n  epsg:\n    type: string\n    inputBinding:\n        prefix: --epsg\n  band:\n    type: string\n    inputBinding:\n        prefix: --band\noutputs:\n  cropped:\n    outputBinding:\n        glob: '*.tif'\n    type: File\n</code></pre> <p>Let's break down the key components of this CWL document:</p> <ul> <li><code>cwlVersion: v1.0</code>: Specifies the version of the CWL specification that this document follows.</li> <li><code>class: CommandLineTool</code>: Indicates that this CWL document defines a command-line tool.</li> <li><code>id: crop</code>: Provides a unique identifier for this tool, which can be used to reference it in workflows.</li> <li><code>requirements</code>: Specifies the requirements and dependencies of the tool. In this case, it defines the following:<ul> <li><code>InlineJavascriptRequirement</code>: This requirement allows the use of inline JavaScript expressions in the tool.</li> <li><code>EnvVarRequirement</code>: It sets environment variables. In this case, it sets the <code>PYTHONPATH</code> environment variable to \"/app.\"</li> <li><code>ResourceRequirement</code>: Specifies resource requirements for running the tool, including the maximum number of CPU cores and maximum RAM.</li> <li><code>DockerRequirement</code>: This requirement specifies the Docker container to be used. It indicates that the tool should be executed in a Docker container with the image <code>localhost/crop:latest</code>.</li> </ul> </li> <li><code>baseCommand</code>: Defines the base command to be executed in the container. In this case, it's running a Python module called \"app\" with the command <code>python -m app</code>.</li> <li><code>arguments</code>: This section is empty, meaning there are no additional command-line arguments specified here. The tool is expected to receive its arguments via the input parameters.</li> <li><code>inputs</code>: Describes the input parameters for the tool, including their types and how they are bound to command-line arguments. The tool expects the following inputs:<ul> <li><code>item</code>: A string representing the input STAC item (image) to be processed, bound to the <code>--input-item</code> argument.</li> <li><code>aoi</code>: A string representing the area of interest (AOI) as a bounding box, bound to the <code>--aoi</code> argument.</li> <li><code>epsg</code>: A string representing the EPSG code for the coordinate system, bound to the <code>--epsg</code> argument.</li> <li><code>band</code>: A string representing the name of the band to be extracted, bound to the <code>--band</code> argument.</li> </ul> </li> <li><code>outputs</code>: Specifies the tool's output. It defines an output parameter named <code>cropped</code>, which is of type <code>File</code>. The outputBinding section specifies that the tool is expected to produce one or more TIFF files (glob: '*.tif') as output.</li> </ul>"},{"location":"cwl-cli/crop/#steps","title":"Steps","text":"<p>Clean-up the <code>/workspace/mastering-app-package/runs</code> folder: </p> <pre><code>rm -fr /workspace/mastering-app-package/runs/*\n</code></pre> <p>Run the CWL document using the <code>cwltool</code> CWL runner to execute the <code>crop</code> step with the <code>green</code> band with:</p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n\ncwltool \\\n    --podman \\\n    --outdir ${WORKSPACE}/runs \\\n    ${WORKSPACE}/cwl-cli/crop.cwl \\\n    --item \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_10TFK_20210713_0_L2A\" \\\n    --aoi=\"-121.399,39.834,-120.74,40.472\" \\\n    --epsg \"EPSG:4326\" \\\n    --band \"green\" \n</code></pre> <pre><code>sh -x ${WORKSPACE}/scripts/cwl-cli-crop-green.sh\n</code></pre> <p>Run the CWL document using the <code>cwltool</code> CWL runner to execute the <code>crop</code> step with the <code>nir</code> band with:</p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n\ncwltool \\\n    --podman \\\n    --outdir ${WORKSPACE}/runs \\\n    ${WORKSPACE}/cwl-cli/crop.cwl \\\n    --item \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_10TFK_20210713_0_L2A\" \\\n    --aoi=\"-121.399,39.834,-120.74,40.472\" \\\n    --epsg \"EPSG:4326\" \\\n    --band \"nir\" \n</code></pre> <pre><code>sh -x ${WORKSPACE}/scripts/cwl-cli-crop-nir.sh\n</code></pre>"},{"location":"cwl-cli/crop/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-mrossi:~/runs$ tree .\n.\n\u251c\u2500\u2500 crop_green.tif\n\u2514\u2500\u2500 crop_nir.tif\n\n0 directories, 2 files\n</code></pre>"},{"location":"cwl-cli/crop/#extra","title":"Extra","text":"<p>The CWL runner <code>cwltool</code> allows you to do a YAML file with the parameters:</p> crop-params.yaml<pre><code>item: \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_10TFK_20210713_0_L2A\"\naoi: \"-121.399,39.834,-120.74,40.472\"\nepsg: \"EPSG:4326\"\nband: \"green\" \n</code></pre> <p>and run it with:</p> terminal<pre><code>cwltool \\\n    --podman \\\n    --outdir /workspace/mastering-app-package/runs \\\n    crop.cwl \\\n    crop-params.yaml \n</code></pre>"},{"location":"cwl-cli/norm-diff/","title":"Normalized difference","text":""},{"location":"cwl-cli/norm-diff/#goal","title":"Goal","text":"<p>Wrap the <code>norm_diff</code> step as a Common Workflow Language CommandLineTool and execute it with a CWL runner.</p>"},{"location":"cwl-cli/norm-diff/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/CommandLineTools/norm-diff.ipynb</p>"},{"location":"cwl-cli/norm-diff/#cwl-commandlinetool-wrapping-the-step","title":"CWL CommandLineTool wrapping the step","text":"<p>The CWL document below shows the <code>norm_diff</code> step wrapped as a CWL CommandLineTool:</p> cwl-cli/norm-diff.cwl<pre><code>cwlVersion: v1.0\n\nclass: CommandLineTool\nid: norm_diff\nrequirements:\n  InlineJavascriptRequirement: {}\n  EnvVarRequirement:\n    envDef:\n      PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n      PYTHONPATH: /app\n  ResourceRequirement:\n    coresMax: 1\n    ramMax: 512\nhints:\n  DockerRequirement:\n    dockerPull: localhost/norm-diff:latest\nbaseCommand: [\"python\", \"-m\", \"app\"]\narguments: []\ninputs:\n  rasters:\n    type: File[]\n    inputBinding:\n      position: 1\noutputs:\n  ndwi:\n    outputBinding:\n      glob: '*.tif'\n    type: File\n</code></pre>"},{"location":"cwl-cli/norm-diff/#steps","title":"Steps","text":"<p>Run the CWL document using the <code>cwltool</code> CWL runner to execute the <code>norm_diff</code> step with:</p> <pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n\ncwltool \\\n    --podman \\\n    --outdir ${WORKSPACE}/runs \\\n    ${WORKSPACE}/cwl-cli/norm-diff.cwl \\\n    --rasters \\\n    ${WORKSPACE}/runs/crop_green.tif \\\n    --rasters \\\n    ${WORKSPACE}/runs/crop_nir.tif\n</code></pre> <pre><code>sh -x ${WORKSPACE}/scripts/cwl-cli-norm-diff.sh\n</code></pre>"},{"location":"cwl-cli/norm-diff/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-mrossi:~/runs$ tree .\n.\n\u251c\u2500\u2500 crop_green.tif\n\u251c\u2500\u2500 crop_nir.tif\n\u2514\u2500\u2500 norm_diff.tif\n\n0 directories, 3 files\n</code></pre>"},{"location":"cwl-cli/otsu/","title":"Ostu threshold","text":""},{"location":"cwl-cli/otsu/#goal","title":"Goal","text":"<p>Wrap the <code>crop</code> step as a Common Workflow Language CommandLineTool and exectute it with a CWL runner.</p>"},{"location":"cwl-cli/otsu/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/CommandLineTools/otsu.ipynb</p>"},{"location":"cwl-cli/otsu/#cwl-commandlinetool-wrapping-the-step","title":"CWL CommandLineTool wrapping the step","text":"<p>The CWL document below shows the <code>crop</code> step wrapped as a CWL CommandLineTool:</p> <pre><code>cwlVersion: v1.0\n\nclass: CommandLineTool\nid: otsu\nrequirements:\n  InlineJavascriptRequirement: {}\n  EnvVarRequirement:\n    envDef:\n      PATH: /opt/conda/envs/env_otsu/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n      PYTHONPATH: /app\n  ResourceRequirement:\n    coresMax: 1\n    ramMax: 512\nhints:\n  DockerRequirement:\n    dockerPull: localhost/otsu:latest \nbaseCommand: [\"python\", \"-m\", \"app\"]\narguments: []\ninputs:\n  raster:\n    type: File\n    inputBinding:\n      position: 1\noutputs:\n  binary_mask_item:\n    outputBinding:\n      glob: '*.tif'\n    type: File\n</code></pre>"},{"location":"cwl-cli/otsu/#steps","title":"Steps","text":"<p>Run the CWL document using the <code>cwltool</code> CWL runner to execute the <code>otsu</code> step with:</p> <pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n\ncwltool \\\n    --podman \\\n    --outdir ${WORKSPACE}/runs \\\n    ${WORKSPACE}/cwl-cli/otsu.cwl \\\n    --raster \\\n    ${WORKSPACE}/runs/norm_diff.tif\n</code></pre> <pre><code>sh -x ${WORKSPACE}/scripts/cwl-cli-otsu.sh\n</code></pre>"},{"location":"cwl-cli/otsu/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-mrossi:~/runs$ tree .\n.\n\u251c\u2500\u2500 crop_green.tif\n\u251c\u2500\u2500 crop_nir.tif\n\u251c\u2500\u2500 norm_diff.tif\n\u2514\u2500\u2500 otsu.tif\n\n0 directories, 4 files\n</code></pre>"},{"location":"cwl-cli/scope/","title":"Scope","text":"<p>When developers package and EO, they are in fact packaging their own software, written in a specific programming language, as a containerized application (or a set of containerized applications), to be described as an EO Application Package using the Common Workflow Language as described in the OGC proposed best practices.</p> <p>To achieve this, developers follow the steps described below.</p> <ul> <li>Prepare one or more container images containing the execution dependencies of the software.</li> <li>Prepare the CWL CommandLineTool document(s) wrapping the command line tool available in container(s).</li> <li>Prepare the CWL Workflow orchestrating CWL CommandLineTool document(s) wrapping the command line tool available container(s).</li> <li>Test the application package in one or more execution scenarios.</li> </ul> <p>This section shows how to do the step:</p> <ul> <li>Prepare the CWL CommandLineTool document(s) wrapping the command line tool available container(s).</li> </ul>"},{"location":"cwl-cli/stac/","title":"STAC Catalog","text":""},{"location":"cwl-cli/stac/#goal","title":"Goal","text":"<p>Wrap the <code>stac</code> step as a Common Workflow Language CommandLineTool and execute it with a CWL runner.</p>"},{"location":"cwl-cli/stac/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/CommandLineTools/stac.ipynb</p>"},{"location":"cwl-cli/stac/#cwl-commandlinetool-wrapping-the-step","title":"CWL CommandLineTool wrapping the step","text":"<p>The CWL document below shows the <code>stac</code> step wrapped as a CWL CommandLineTool:</p> <pre><code>cwlVersion: v1.0\n\nclass: CommandLineTool\nid: stac\nrequirements:\n  InlineJavascriptRequirement: {}\n  EnvVarRequirement:\n    envDef:\n      PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n      PYTHONPATH: /app\n  ResourceRequirement:\n    coresMax: 1\n    ramMax: 512\nhints:\n  DockerRequirement:\n    dockerPull: localhost/stac:latest \nbaseCommand: [\"python\", \"-m\", \"app\"]\narguments: []\ninputs:\n  item:\n    type: string\n    inputBinding:\n      prefix: --input-item\n  raster:\n    type: File\n    inputBinding:\n      prefix: --water-body\noutputs:\n  stac_catalog:\n    outputBinding:\n      glob: .\n    type: Directory\n</code></pre>"},{"location":"cwl-cli/stac/#steps","title":"Steps","text":"<p>Run the CWL document using the <code>cwltool</code> CWL runner to execute the <code>stac</code> step with:</p> <pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n\ncwltool \\\n    --podman \\\n    --outdir ${WORKSPACE}/runs \\\n    ${WORKSPACE}/cwl-cli/stac.cwl \\\n    --item \\\n    \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_10TFK_20210713_0_L2A\" \\\n    --raster \\\n    ${WORKSPACE}/runs/otsu.tif\n</code></pre> <pre><code>sh -x ${WORKSPACE}/scripts/cwl-cli-stac.sh\n</code></pre>"},{"location":"cwl-cli/stac/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-fbrito:~/runs$ tree .\n.\n\u251c\u2500\u2500 crop_green.tif\n\u251c\u2500\u2500 crop_nir.tif\n\u251c\u2500\u2500 norm_diff.tif\n\u251c\u2500\u2500 otsu.tif\n\u2514\u2500\u2500 p371fwrs\n    \u251c\u2500\u2500 S2B_10TFK_20210713_0_L2A\n    \u2502   \u251c\u2500\u2500 S2B_10TFK_20210713_0_L2A.json\n    \u2502   \u2514\u2500\u2500 otsu.tif\n    \u2514\u2500\u2500 catalog.json\n\n2 directories, 7 files\n</code></pre> <p>Line #6 shows the folder created by the execution.</p>"},{"location":"cwl-workflow/cloud-native/","title":"Sentinel-2 Cloud native Workflow","text":"<p>The Cloud native Workflow chains the <code>crop</code>, <code>norm_diff</code>, <code>otsu</code> and <code>stac</code> steps taking a single STAC item as input parameters:</p> <ul> <li>a SpatioTemporal Asset Catalog (STAC) Item</li> <li>a bounding box area of interest (AOI)</li> <li>the EPSG code of the bounding box area of interest</li> <li>a list of common band names ([\"green\", \"nir\"])</li> </ul> graph TB A[STAC Item URL] A --&gt; B((\"crop(green)\")); A--&gt; C((\"crop(nir)\")); A[STAC Item URL] --&gt; F P[bands] Q[EPSG code] R[AOI] subgraph scatter on bands   P --&gt; B((\"crop(green)\"))   P --&gt; C((\"crop(nir)\"))   Q --&gt; B((\"crop(green)\"))   Q --&gt; C((\"crop(nir)\"))   R --&gt; B((\"crop(green)\"))   R --&gt; C((\"crop(nir)\")) end B((\"crop(green)\")) --&gt; D C((\"crop(nir)\")) --&gt; D D((\"`Normalized  difference`\")); D --&gt; E((\"`Otsu  threshold`\")) E --&gt; F((\"`Create  STAC Catalog`\")) <p>The CWL Workflow is shown below and the lines highlighted chain the steps:</p> app-water-body-cloud-native.cwl<pre><code>cwlVersion: v1.0\n$namespaces:\n  s: https://schema.org/\ns:softwareVersion: 1.0.0\nschemas:\n  - http://schema.org/version/9.0/schemaorg-current-http.rdf\n$graph:\n  - class: Workflow\n    id: water-bodies\n    label: Water bodies detection based on NDWI and the otsu threshold\n    doc: Water bodies detection based on NDWI and otsu threshold applied to a single Sentinel-2 COG STAC item\n    requirements:\n      - class: ScatterFeatureRequirement\n    inputs:\n      aoi:\n        label: area of interest\n        doc: area of interest as a bounding box\n        type: string\n      epsg:\n        label: EPSG code\n        doc: EPSG code\n        type: string\n        default: \"EPSG:4326\"\n      bands:\n        label: bands used for the NDWI\n        doc: bands used for the NDWI\n        type: string[]\n        default: [\"green\", \"nir\"]\n      item:\n        doc: Reference to a STAC item\n        label: STAC item reference\n        type: string\n    outputs:\n      - id: stac_catalog\n        outputSource:\n          - node_stac/stac_catalog\n        type: Directory\n    steps:\n      node_crop:\n        run: \"#crop\"\n        in:\n          item: item\n          aoi: aoi\n          epsg: epsg\n          band: bands\n        out:\n          - cropped\n        scatter: band\n        scatterMethod: dotproduct\n      node_normalized_difference:\n        run: \"#norm_diff\"\n        in:\n          rasters:\n            source: node_crop/cropped\n        out:\n          - ndwi\n      node_otsu:\n        run: \"#otsu\"\n        in:\n          raster:\n            source: node_normalized_difference/ndwi\n        out:\n          - binary_mask_item\n      node_stac:\n        run: \"#stac\"\n        in:\n          item: item\n          rasters:\n            source: node_otsu/binary_mask_item\n        out:\n          - stac_catalog\n  - class: CommandLineTool\n    id: crop\n    requirements:\n      InlineJavascriptRequirement: {}\n      EnvVarRequirement:\n        envDef:\n          PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n          PYTHONPATH: /app\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 512\n    hints:\n      DockerRequirement:\n        dockerPull: localhost/crop:latest\n    baseCommand: [\"python\", \"-m\", \"app\"]\n    arguments: []\n    inputs:\n      item:\n        type: string\n        inputBinding:\n          prefix: --input-item\n      aoi:\n        type: string\n        inputBinding:\n          prefix: --aoi\n      epsg:\n        type: string\n        inputBinding:\n          prefix: --epsg\n      band:\n        type: string\n        inputBinding:\n          prefix: --band\n    outputs:\n      cropped:\n        outputBinding:\n          glob: '*.tif'\n        type: File\n  - class: CommandLineTool\n    id: norm_diff\n    requirements:\n      InlineJavascriptRequirement: {}\n      EnvVarRequirement:\n        envDef:\n          PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n          PYTHONPATH: /app\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 512\n    hints:\n      DockerRequirement:\n        dockerPull: localhost/norm-diff:latest\n    baseCommand: [\"python\", \"-m\", \"app\"]\n    arguments: []\n    inputs:\n      rasters:\n        type: File[]\n        inputBinding:\n          position: 1\n    outputs:\n      ndwi:\n        outputBinding:\n          glob: '*.tif'\n        type: File\n  - class: CommandLineTool\n    id: otsu\n    requirements:\n      InlineJavascriptRequirement: {}\n      EnvVarRequirement:\n        envDef:\n          PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n          PYTHONPATH: /app\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 512\n    hints:\n      DockerRequirement:\n        dockerPull: localhost/otsu:latest\n    baseCommand: [\"python\", \"-m\", \"app\"]\n    arguments: []\n    inputs:\n      raster:\n        type: File\n        inputBinding:\n          position: 1\n    outputs:\n      binary_mask_item:\n        outputBinding:\n          glob: '*.tif'\n        type: File\n  - class: CommandLineTool\n    id: stac\n    requirements:\n      InlineJavascriptRequirement: {}\n      EnvVarRequirement:\n        envDef:\n          PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n          PYTHONPATH: /app\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 512\n    hints:\n      DockerRequirement:\n        dockerPull: localhost/stac:latest\n    baseCommand: [\"python\", \"-m\", \"app\"]\n    arguments: []\n    inputs:\n      item:\n        type: string\n        inputBinding:\n          prefix: --input-item\n      rasters:\n        type: File\n        inputBinding:\n          prefix: --water-body\n    outputs:\n      stac_catalog:\n        outputBinding:\n          glob: .\n        type: Directory\n</code></pre>"},{"location":"cwl-workflow/exec-cloud-native/","title":"Sentinel-2","text":""},{"location":"cwl-workflow/exec-cloud-native/#goal","title":"Goal","text":"<p>Run the <code>app-water-body-cloud-native.1.0.0.cwl</code> released application package using <code>cwltool</code>.</p>"},{"location":"cwl-workflow/exec-cloud-native/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/Workflows/cloud-native.ipynb</p>"},{"location":"cwl-workflow/exec-cloud-native/#step-1-configure-the-workspace","title":"Step 1 - Configure the workspace","text":"<p>The results produced will be available in the local folder <code>/workspace/mastering-app-package/runs</code></p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n</code></pre> <pre><code>source /workspace/mastering-app-package/scripts/setup.sh\n</code></pre>"},{"location":"cwl-workflow/exec-cloud-native/#step-2-download-the-released-application-package","title":"Step 2 - Download the released Application package","text":"scripts/download-app-water-body-cloud-native.sh<pre><code>version=\"1.0.0\"\n\nwget \\\n    -O ${WORKSPACE}/runs/app-water-body-cloud-native.${version}.cwl \\\n    https://github.com/eoap/mastering-app-package/releases/download/${version}/app-water-body-cloud-native.${version}.cwl\n</code></pre> <pre><code>sh ${WORKSPACE}/scripts/download-app-water-body-cloud-native.sh\n</code></pre>"},{"location":"cwl-workflow/exec-cloud-native/#step-3-execute-the-application-package","title":"Step 3 - Execute the Application Package","text":"scripts/exec-app-water-body-cloud-native.sh<pre><code>version=\"1.0.0\"\n\ncwltool \\\n    --podman \\\n    --outdir ${WORKSPACE}/runs \\\n    ${WORKSPACE}/runs/app-water-body-cloud-native.${version}.cwl \\\n    --item \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_10TFK_20210713_0_L2A\" \\\n    --aoi=\"-121.399,39.834,-120.74,40.472\" \\\n    --epsg \"EPSG:4326\"\n</code></pre> <pre><code>sh ${WORKSPACE}/scripts/exec-app-water-body-cloud-native.sh\n</code></pre>"},{"location":"cwl-workflow/exec-cloud-native/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-fbrito:~/runs$ tree .\n/workspace/mastering-app-package/runs/\n\u251c\u2500\u2500 app-water-body-cloud-native.1.0.0.cwl\n\u2514\u2500\u2500 cmtriamc\n    \u251c\u2500\u2500 S2B_10TFK_20210713_0_L2A\n    \u2502   \u251c\u2500\u2500 S2B_10TFK_20210713_0_L2A.json\n    \u2502   \u2514\u2500\u2500 otsu.tif\n    \u2514\u2500\u2500 catalog.json\n\n2 directories, 4 files\n</code></pre>"},{"location":"cwl-workflow/exec-stage-in/","title":"Landsat-9","text":""},{"location":"cwl-workflow/exec-stage-in/#goal","title":"Goal","text":"<p>Run the <code>app-water-body.1.0.0.cwl</code> released application package using <code>cwltool</code> on a staged Landsat-9 acquisition.</p>"},{"location":"cwl-workflow/exec-stage-in/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/Workflows/staged.ipynb</p>"},{"location":"cwl-workflow/exec-stage-in/#step-1-configure-the-workspace","title":"Step 1 - Configure the workspace","text":"<p>The results produced will be available in the local folder <code>/workspace/mastering-app-package/runs</code></p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n</code></pre> <pre><code>source /workspace/mastering-app-package/scripts/setup.sh\n</code></pre>"},{"location":"cwl-workflow/exec-stage-in/#step-2-download-the-released-application-package","title":"Step 2 - Download the released Application package","text":"scripts/download-app-water-body.sh<pre><code>version=\"1.0.0\"\n\nwget \\\n    -O ${WORKSPACE}/runs/app-water-body.${version}.cwl \\\n    https://github.com/eoap/mastering-app-package/releases/download/${version}/app-water-body.${version}.cwl\n</code></pre> <pre><code>sh ${WORKSPACE}/scripts/download-app-water-body.sh\n</code></pre>"},{"location":"cwl-workflow/exec-stage-in/#step-3-stage-in-the-landsat-9-acquisition","title":"Step 3 - Stage-in the Landsat-9 acquisition","text":"<p>The Landsat-9 acquistion \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/landsat-c2-l2/items/LC09_L2SP_042033_20231015_02_T1\" is staged with: </p> <pre><code>ls9_ref=\"https://planetarycomputer.microsoft.com/api/stac/v1/collections/landsat-c2-l2/items/LC09_L2SP_042033_20231015_02_T1\"\n\ncwltool \\\n    --podman \\\n    --outdir ${WORKSPACE}/runs \\\n    ${WORKSPACE}/cwl-cli/stage-in-released.cwl \\\n    --reference \\\n    ${ls9_ref} &gt; staged.json\n</code></pre> <pre><code>sh ${WORKSPACE}/scripts/cwl-cli-stage-in-released.sh\n</code></pre> <p>The expected content after this step is:</p> <pre><code>(base) jovyan@coder-fbrito:~/runs$ tree\n.\n\u251c\u2500\u2500 app-water-body.1.0.0.cwl\n\u251c\u2500\u2500 lwft9j0o\n\u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_02_T1\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_02_T1.json\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ANG.txt\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_MTL.json\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_MTL.txt\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_MTL.xml\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_QA_PIXEL.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_QA_RADSAT.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B1.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B2.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B3.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B4.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B5.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B6.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B7.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_QA_AEROSOL.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_ATRAN.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_B10.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_CDIST.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_DRAD.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_EMIS.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_EMSD.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_QA.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_TRAD.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_URAD.TIF\n\u2502   \u2502   \u251c\u2500\u2500 preview.png\n\u2502   \u2502   \u2514\u2500\u2500 tilejson.json\n\u2502   \u251c\u2500\u2500 catalog.json\n\u2502   \u2514\u2500\u2500 stage.py\n\u2514\u2500\u2500 staged.json\n\n2 directories, 30 files\n</code></pre> <p>The result is redirected to a file named <code>staged.json</code> as we use <code>jq</code> to get the path of the staged product:</p> terminal<pre><code>cat staged.json | jq -r .staged.path\n</code></pre> <p>This returns a path like <code>/workspace/mastering-app-package/runs/921x91vw</code></p>"},{"location":"cwl-workflow/exec-stage-in/#step-4-execute-the-application-package","title":"Step 4 - Execute the Application Package","text":"<p>Use <code>cwltool</code> to run the application package using the staged Landsat-9 acquisition:</p> <pre><code>version=\"1.0.0\"\n\ncwltool \\\n    --podman \\\n    --outdir ${WORKSPACE}/runs \\\n    ${WORKSPACE}/runs/app-water-body.${version}.cwl \\\n    --item $( cat staged.json | jq -r .staged.path ) \\\n    --aoi=\"-118.985,38.432,-118.183,38.938\" \\\n    --epsg \"EPSG:4326\" \\\n    --bands green \\\n    --bands nir08 &gt; results.json\n</code></pre> <pre><code>sh ${WORKSPACE}/scripts/exec-app-water-body.sh\n</code></pre>"},{"location":"cwl-workflow/exec-stage-in/#step-5-stage-out-to-s3","title":"Step 5 - Stage-out to S3","text":"<p>This step uploads the STAC Catalog to S3.</p> terminal<pre><code>cwltool \\\n    --podman \\\n    ${WORKSPACE}/cwl-cli/stage-out-released.cwl \\\n    --aws_access_key_id $( cat ~/.aws/credentials | grep aws_access_key_id | cut -d \"=\" -f 2 ) \\\n    --aws_secret_access_key $( cat ~/.aws/credentials | grep aws_secret_access_key | cut -d \"=\" -f 2 ) \\\n    --endpoint_url $( cat ~/.aws/config | grep endpoint_url | head -n 1 | cut -d \"=\" -f 2 ) \\\n    --region_name $( cat ~/.aws/config | grep region | cut -d \"=\" -f 2 ) \\\n    --s3_bucket processing-results \\\n    --sub_path processing-results/$( cat /proc/sys/kernel/random/uuid ) \\\n    --stac_catalog /workspace/mastering-app-package/runs\n</code></pre> <pre><code>sh ${WORKSPACE}/scripts/cwl-cli-stage-out-released.sh\n</code></pre> <p>Check the staged results on S3:</p> terminal<pre><code>aws s3 ls $( cat staged-s3.json | jq -r .s3_catalog_output | xargs dirname )/\n</code></pre>"},{"location":"cwl-workflow/exec-stage-in/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-mrossi:~/runs$ tree .\n.\n\u251c\u2500\u2500 app-water-body.1.0.0.cwl\n\u251c\u2500\u2500 cg8n2dnu\n\u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_02_T1\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_02_T1.json\n\u2502   \u2502   \u2514\u2500\u2500 otsu.tif\n\u2502   \u2514\u2500\u2500 catalog.json\n\u251c\u2500\u2500 lwft9j0o\n\u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_02_T1\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_02_T1.json\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ANG.txt\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_MTL.json\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_MTL.txt\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_MTL.xml\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_QA_PIXEL.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_QA_RADSAT.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B1.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B2.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B3.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B4.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B5.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B6.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_B7.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_SR_QA_AEROSOL.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_ATRAN.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_B10.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_CDIST.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_DRAD.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_EMIS.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_EMSD.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_QA.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_TRAD.TIF\n\u2502   \u2502   \u251c\u2500\u2500 LC09_L2SP_042033_20231015_20231016_02_T1_ST_URAD.TIF\n\u2502   \u2502   \u251c\u2500\u2500 preview.png\n\u2502   \u2502   \u2514\u2500\u2500 tilejson.json\n\u2502   \u251c\u2500\u2500 catalog.json\n\u2502   \u2514\u2500\u2500 stage.py\n\u2514\u2500\u2500 staged.json\n\n4 directories, 33 files\n</code></pre> <p>and </p> <pre><code>(base) jovyan@coder-mrossi:~/runs$ aws s3 ls $( cat staged-s3.json | jq -r .s3_catalog_output | xargs dirname )/\n                           PRE LC09_L2SP_042033_20231015_02_T1/\n2023-11-02 08:46:04        549 catalog.json\n</code></pre>"},{"location":"cwl-workflow/scatter-cloud-native/","title":"Sentinel-2 Workflow of Workflow","text":"<p>CWL can run sub-workflows in a step. </p> <p>To process a list of STAC Items and then generate a STAC catalog with several detected water bodies, the flowchart is:</p> graph TB A[\"[STAC Item URL, STAC Item URL]\"] A --&gt; F A --&gt; B((\"crop(green)\")); A--&gt; C((\"crop(nir)\")); subgraph scatter on STAC Items P[bands] Q[EPSG code] R[AOI] subgraph scatter on bands   P --&gt; B((\"crop(green)\"))   P --&gt; C((\"crop(nir)\"))   Q --&gt; B((\"crop(green)\"))   Q --&gt; C((\"crop(nir)\"))   R --&gt; B((\"crop(green)\"))   R --&gt; C((\"crop(nir)\")) end B((\"crop(green)\")) --&gt; D C((\"crop(nir)\")) --&gt; D D((\"`Normalized  difference`\")); D --&gt; E((\"`Otsu  threshold`\")) end E --&gt; F F((\"`Create  STAC Catalog`\")) <p>Below a CWL Workflow implementing this scenario:</p> <pre><code>  - class: Workflow\n    id: water-bodies\n    label: Water bodies detection based on NDWI and otsu threshold\n    doc: Water bodies detection based on NDWI and otsu threshold applied to Sentinel-2 COG STAC items\n    requirements:\n      - class: ScatterFeatureRequirement\n      - class: SubworkflowFeatureRequirement\n    inputs:\n      aoi:\n        label: area of interest\n        doc: area of interest as a bounding box\n        type: string\n      epsg:\n        label: EPSG code\n        doc: EPSG code\n        type: string\n        default: \"EPSG:4326\"\n      stac_items:\n        label: Sentinel-2 STAC items\n        doc: list of Sentinel-2 COG STAC items\n        type: string[]\n      bands:\n        label: bands used for the NDWI\n        doc: bands used for the NDWI\n        type: string[]\n        default: [\"green\", \"nir\"]\n    outputs:\n      - id: stac_catalog\n        outputSource:\n          - node_stac/stac_catalog\n        type: Directory\n    steps:\n      node_water_bodies:\n        run: \"#detect_water_body\"\n        in:\n          item: stac_items\n          aoi: aoi\n          epsg: epsg\n          bands: bands\n        out:\n          - detected_water_body\n        scatter: item\n        scatterMethod: dotproduct\n      node_stac:\n        run: \"#stac\"\n        in:\n          item: stac_items\n          rasters:\n            source: node_water_bodies/detected_water_body\n        out:\n          - stac_catalog\n</code></pre> <p>The <code>stac</code> CommandLineTool is updated to manage arrays:</p> <pre><code>  - class: CommandLineTool\n    id: stac\n    requirements:\n      InlineJavascriptRequirement: {}\n      EnvVarRequirement:\n        envDef:\n          PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n          PYTHONPATH: /app\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 512\n    hints:\n      DockerRequirement:\n        dockerPull: localhost/stac:latest\n    baseCommand: [\"python\", \"-m\", \"app\"]\n    arguments: []\n    inputs:\n      item:\n        type:\n          type: array\n          items: string\n          inputBinding:\n            prefix: --input-item\n      rasters:\n        type:\n          type: array\n          items: File\n          inputBinding:\n            prefix: --water-body\n    outputs:\n      stac_catalog:\n        outputBinding:\n          glob: .\n        type: Directory\n</code></pre> <p>To run this CWL document, one would do:</p> <pre><code>export WORKSPACE=/workspace/mastering-app-package\n\ncwltool \\\n    --podman \\\n    --outdir${WORKSPACE}/runs \\\n    ${WORKSPACE}/cwl-workflow/app-water-bodies-cloud-native.cwl \\\n    --stac_items \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_10TFK_20210713_0_L2A\" \\\n    --stac_items \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2A_10TFK_20220524_0_L2A\" \\\n    --aoi=\"-121.399,39.834,-120.74,40.472\" \\\n    --epsg \"EPSG:4326\"\n</code></pre>"},{"location":"cwl-workflow/scope/","title":"Scope","text":"<p>When developers package and EO, they are in fact packaging their own software, written in a specific programming language, as a containerized application (or a set of containerized applications), to be described as an EO Application Package using the Common Workflow Language as described in the OGC proposed best practices.</p> <p>To achieve this, developers follow the steps described below.</p> <ul> <li>Prepare one or more container images containing the execution dependencies of the software.</li> <li>Prepare the CWL CommandLineTool document(s) wrapping the command line tool available in container(s).</li> <li>Prepare the CWL Workflow orchestrating CWL CommandLineTool document(s) wrapping the command line tool available container(s).</li> <li>Test the application package in one or more execution scenarios.</li> </ul> <p>This section shows how to do the step:</p> <ul> <li>Prepare the CWL Workflow orchestrating CWL CommandLineTool document(s) wrapping the command line tool available container(s).</li> </ul>"},{"location":"cwl-workflow/scope/#orchestrating-cwl-commandlinetools","title":"Orchestrating CWL CommandLineTools","text":"<p>The CWL CommandLineTools wrapping the <code>crop</code>, <code>norm_diff</code>, <code>otsu</code> and <code>stac</code> command-line tools had a single CWL class in each of the CWL documents.</p> <p>For orchestrating these CWL CommandLineTools, we will need a CWL document with a list of process objects. </p> <p>A <code>$graph</code> document does not have a process object at the root. Instead there is a $graph field which consists of a list of process objects.</p> <p>Each process object must have an <code>id</code> field: </p> <pre><code>cwlVersion=1.0\n\n$graph:\n- class: Workflow\n  id: main\n  ...\n\n- class: CommandLineTool\n  id: crop\n  ...\n\n- class: CommandLineTool\n  id: norm_diff\n  ...\n\n- class: CommandLineTool\n  id: otsu\n  ...\n\n- class: CommandLineTool\n  id: norm_diff\n  ...\n</code></pre> <p>Workflow run fields cross-reference other processes in the document <code>$graph</code> using the <code>id</code> of the process object:</p> <pre><code>cwlVersion=1.0\n\n$graph:\n- class: Workflow\n  id: main\n  steps:\n    node_crop:\n      run: \"#crop\"\n      ...\n    node_normalized_difference:\n      run: \"#norm_diff\"\n      ...\n    node_otsu:\n      run: \"#otsu\"\n      ...\n    node_stac:\n      run: \"#stac\"\n      ...\n  ...\n\n- class: CommandLineTool\n  id: crop\n  ...\n\n- class: CommandLineTool\n  id: norm_diff\n  ...\n\n- class: CommandLineTool\n  id: otsu\n  ...\n\n- class: CommandLineTool\n  id: norm_diff\n  ...\n</code></pre> <p>We propose three Workflows:</p> <ul> <li><code>app-water-body-cloud-native.cwl</code> reading a STAC Item as input and orchestrating the <code>crop</code>, <code>norm_diff</code>, <code>otsu</code> and <code>stac</code> steps and thus taking a single STAC item as input</li> <li><code>app-water-body.cwl</code> reading a staged EO acquisition as a STAC Catalog </li> <li><code>app-water-bodies-cloud-native.cwl</code> that scatters the Workflow <code>app-water-body-cloud-native.cwl</code> on a list of STAC items.</li> </ul>"},{"location":"cwl-workflow/stage-in-out/","title":"Understanding stage-in/out","text":""},{"location":"cwl-workflow/stage-in-out/#stage-in","title":"Stage-in","text":"<p>From the OGC Best Practice for Earth Observation Application Package:</p> <p>An Application input argument that requires staged EO product files SHALL be defined as an argument that points to a folder where a STAC Catalog, named catalog.json, contains a list of one or more STAC Items and associated STAC Assets referencing the files.</p> <p>This translates to:</p> <ul> <li>Plug a stage-in step for all workflow steps having inputs of type <code>Directory</code></li> <li>Workflow steps having inputs of type <code>Directory</code> will find a STAC catalog.json file</li> </ul>"},{"location":"cwl-workflow/stage-in-out/#stage-out","title":"Stage-out","text":"<p>From the OGC Best Practice for Earth Observation Application Package:</p> <p>An Application that creates EO product files to be stage-out SHALL generate a valid STAC Catalog, named catalog.json, and include the STAC Item(s) and corresponding STAC Assets pointing to the results of the processing.</p> <p>The STAC Catalog created by the Application SHALL include metadata elements for each STAC Item with at least their spatial (geometry, box) and temporal (datetime) properties.</p> <p>This translates to:</p> <ul> <li>Workflow steps that have an output of type <code>Directory</code> produce a STAC catalog</li> <li>Plug a stage-out step for all workflow outputs of type Directory</li> </ul>"},{"location":"cwl-workflow/stage-in-out/#applying-the-stage-inout-to-the-water-bodies","title":"Applying the stage-in/out to the water bodies","text":"<p>The concepts above mapped to the Water Body Detection application are depicted below.</p> graph TB style AA stroke:#f66,stroke-width:3px style BB stroke:#f66,stroke-width:3px subgraph stage-in A[STAC Item] -- STAC Item URL --&gt; AA[Stage-in] AA[Stage-in] -- catalog.json/item.json/assets blue, red,  nir ... --&gt; AB[(local storage)] end subgraph Process STAC item AB[(storage)] -- Staged STAC Catalog --&gt; B AB[(storage)] -- Staged STAC Catalog --&gt; C AB[(storage)] -- Staged STAC Catalog --&gt; F subgraph scatter on bands B[\"crop(green)\"]; C[\"crop(nir)\"]; end B[\"crop(green)\"] -.-&gt; D[Normalized difference]; C[\"crop(nir)\"] -.-&gt; D[Normalized difference]; D -.-&gt; E[Otsu threshold] end E -.-&gt; F[Create STAC Catalog] F -.-&gt; G[(storage)]  subgraph stage-out  G -- \"catalog.json/item.json/asset otsu.tif\" --&gt; BB[Stage-out]  BB --&gt; H[(\"Remote   storage\")] end"},{"location":"cwl-workflow/stage-in/","title":"CWL CommandLineTool for stage-in","text":""},{"location":"cwl-workflow/stage-in/#goal","title":"Goal","text":"<p>Use a stage-in CWL workflow to stage a Landsat-9 acquisition</p>"},{"location":"cwl-workflow/stage-in/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/Workflows/stage-in.ipynb</p>"},{"location":"cwl-workflow/stage-in/#step-1-create-a-stage-in-cwl-workflow","title":"Step 1 - Create a stage-in CWL Workflow","text":"<p>Below a <code>stage-in.cwl</code> CWL (Common Workflow Language) document for a command-line tool that executes a Python script. </p> <p>The document is designed to use a container and execute a Python script named \"stage.py\":</p> stage-in.cwl<pre><code>cwlVersion: v1.0\n\nclass: CommandLineTool\nid: main\ninputs:\n  reference:\n    type: string\noutputs:\n  staged:\n    type: Directory\n    outputBinding:\n      glob: .\nbaseCommand: \n- python\n- stage.py\narguments:\n- $( inputs.reference )\nrequirements:\n  DockerRequirement:\n    dockerPull: localhost/stage:latest\n  InlineJavascriptRequirement: {}\n  InitialWorkDirRequirement:\n    listing:\n      - entryname: stage.py\n        entry: |-\n          import pystac\n          import stac_asset\n          import asyncio\n          import os\n          import sys\n\n          config = stac_asset.Config(warn=True)\n\n          async def main(href: str):\n\n              item = pystac.read_file(href)\n\n              os.makedirs(item.id, exist_ok=True)\n              cwd = os.getcwd()\n\n              os.chdir(item.id)\n              item = await stac_asset.download_item(item=item, directory=\".\", config=config)\n              os.chdir(cwd)\n\n              cat = pystac.Catalog(\n                  id=\"catalog\",\n                  description=f\"catalog with staged {item.id}\",\n                  title=f\"catalog with staged {item.id}\",\n              )\n              cat.add_item(item)\n\n              cat.normalize_hrefs(\"./\")\n              cat.save(catalog_type=pystac.CatalogType.SELF_CONTAINED)\n\n              return cat\n\n          href = sys.argv[1]\n\n          cat = asyncio.run(main(href))\n</code></pre> <p>The <code>stage.py</code> Python script uses the <code>pystac</code> and <code>stac_asset</code> libraries to stage STAC Items, download them, and create a STAC Catalog.  It's designed to be run from the command line, taking the URL of a STAC Item as a command-line argument. </p> <p>Here's a breakdown of what the script does:</p> <ul> <li> <p>The script imports the necessary libraries: <code>pystac</code>, <code>stac_asset</code>, <code>asyncio</code>, <code>os</code>, and <code>sys</code>.</p> </li> <li> <p>It sets up the configuration for the stac_asset library with <code>warn=True</code>.</p> </li> <li> <p>The script defines an <code>async</code> function named <code>main</code>, which takes a STAC Item URL (href) as an argument.</p> </li> <li> <p>Inside the <code>main</code> function:</p> <ul> <li>It reads the STAC Item using <code>pystac.read_file(href)</code> and stores it in the item variable.</li> <li>It creates a directory with the same name as the STAC Item's ID if it doesn't exist using <code>os.makedirs(item.id, exist_ok=True)</code>.</li> <li>It temporarily changes the current working directory to the newly created directory using <code>os.chdir(item.id)</code>.</li> <li>It downloads the STAC Item's assets to the current directory using <code>stac_asset.download_item(item=item, directory=\".\", config=config)</code>.</li> <li>It returns the STAC Item.</li> </ul> </li> </ul> <p>After defining the <code>main</code> function, the script sets the href variable to the first command-line argument provided (<code>sys.argv[1]</code>).</p> <ul> <li> <p>It uses <code>asyncio.run</code> to execute the main function asynchronously with the specified href.</p> </li> <li> <p>In the <code>main</code> function, a new STAC Catalog is created:</p> <ul> <li>It's given the ID \"catalog\" and a description and title that mention the staged STAC Item.</li> <li>The staged STAC Item is added to the catalog.</li> <li>The catalog's hrefs are normalized relative to the current working directory.</li> <li>The catalog is saved as a self-contained catalog.</li> </ul> </li> </ul>"},{"location":"cwl-workflow/stage-in/#step-2-create-a-container-for-the-stage-in","title":"Step 2 - Create a container for the stage-in","text":"<p>The container image is built with: </p> <pre><code>export WORKSPACE=/workspace/mastering-app-package\n\npodman \\\n    build \\\n    --format docker \\\n    -t localhost/stage:latest \\\n    ${WORKSPACE}/water-bodies/command-line-tools/stage\n</code></pre>"},{"location":"cwl-workflow/stage-in/#step-3-stage-the-landsat-9-acquisition","title":"Step 3 - Stage the Landsat-9 acquisition","text":"<p>Now the Landsat-9 acquisition \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/landsat-c2-l2/items/LC09_L2SP_042033_20231015_02_T1\" is staged with: </p> <pre><code>ls9_ref=\"https://planetarycomputer.microsoft.com/api/stac/v1/collections/landsat-c2-l2/items/LC09_L2SP_042033_20231015_02_T1\"\n\ncwltool \\\n    --podman \\\n    --outdir ${WORKSPACE}/runs \\\n    ${WORKSPACE}/cwl-cli/stage-in.cwl \\\n    --reference \\\n    ${ls9_ref} &gt; staged.json\n</code></pre>"},{"location":"cwl-workflow/stage-in/#step-4-check-the-path-to-the-stage-landsat-9-acquisition","title":"Step 4 - Check the path to the stage Landsat-9 acquisition","text":"<p>The result is redirected to a file named <code>staged.json</code> as we use <code>jq</code> to get the path of the staged product:</p> terminal<pre><code>cat staged.json | jq -r .staged.path\n</code></pre> <p>This returns a path like <code>/workspace/mastering-app-package/runs/921x91vw</code></p>"},{"location":"cwl-workflow/stage-out/","title":"CWL CommandLineTool for stage-out","text":"<p>Below a <code>stage-out.cwl</code> CWL (Common Workflow Language) document for a command-line tool that executes a Python script. </p> <p>The document is designed to use a container and execute a Python script named \"stage.py\":</p> <pre><code>cwlVersion: v1.0\n\n\nclass: CommandLineTool\nid: stage-out\n\ndoc: \"Stage-out the results to S3\"\ninputs:\n  s3_bucket:\n    type: string\n  sub_path:\n    type: string\n  aws_access_key_id:\n    type: string\n  aws_secret_access_key:\n    type: string\n  region_name:\n    type: string\n  endpoint_url:\n    type: string\n  stac_catalog:\n    type: Directory\noutputs:\n  s3_catalog_output:\n    outputBinding:\n      outputEval: ${  return \"s3://\" + inputs.s3_bucket + \"/\" + inputs.sub_path + \"/catalog.json\"; }\n    type: string\nbaseCommand:\n  - python\n  - stage.py\narguments:\n  - $( inputs.stac_catalog.path )\n  - $( inputs.s3_bucket )\n  - $( inputs.sub_path )\nrequirements:\n  DockerRequirement:\n    dockerPull: ghcr.io/terradue/ogc-eo-application-package-hands-on/stage:1.3.2\n  InlineJavascriptRequirement: {}\n  EnvVarRequirement:\n    envDef:\n      aws_access_key_id: $( inputs.aws_access_key_id )\n      aws_secret_access_key: $( inputs.aws_secret_access_key )\n      aws_region_name: $( inputs.region_name )\n      aws_endpoint_url: $( inputs.endpoint_url )\n  ResourceRequirement: {}\n  InitialWorkDirRequirement:\n    listing:\n      - entryname: stage.py\n        entry: |-\n          import os\n          import sys\n          import pystac\n          import botocore\n          import boto3\n          import shutil\n          from pystac.stac_io import DefaultStacIO, StacIO\n          from urllib.parse import urlparse\n\n          cat_url = sys.argv[1]\n          bucket = sys.argv[2]\n          subfolder = sys.argv[3]\n\n          aws_access_key_id = os.environ[\"aws_access_key_id\"]\n          aws_secret_access_key = os.environ[\"aws_secret_access_key\"]\n          region_name = os.environ[\"aws_region_name\"]\n          endpoint_url = os.environ[\"aws_endpoint_url\"]\n\n          shutil.copytree(cat_url, \"/tmp/catalog\")\n          cat = pystac.read_file(os.path.join(\"/tmp/catalog\", \"catalog.json\"))\n\n          class CustomStacIO(DefaultStacIO):\n              \"\"\"Custom STAC IO class that uses boto3 to read from S3.\"\"\"\n\n              def __init__(self):\n                  self.session = botocore.session.Session()\n                  self.s3_client = self.session.create_client(\n                      service_name=\"s3\",\n                      use_ssl=True,\n                      aws_access_key_id=aws_access_key_id,\n                      aws_secret_access_key=aws_secret_access_key,\n                      endpoint_url=endpoint_url,\n                      region_name=region_name,\n                  )\n\n              def write_text(self, dest, txt, *args, **kwargs):\n                  parsed = urlparse(dest)\n                  if parsed.scheme == \"s3\":\n                      self.s3_client.put_object(\n                          Body=txt.encode(\"UTF-8\"),\n                          Bucket=parsed.netloc,\n                          Key=parsed.path[1:],\n                          ContentType=\"application/geo+json\",\n                      )\n                  else:\n                      super().write_text(dest, txt, *args, **kwargs)\n\n\n          client = boto3.client(\n              \"s3\",\n              aws_access_key_id=aws_access_key_id,\n              aws_secret_access_key=aws_secret_access_key,\n              endpoint_url=endpoint_url,\n              region_name=region_name,\n          )\n\n          StacIO.set_default(CustomStacIO)\n\n          for item in cat.get_items():\n              for key, asset in item.get_assets().items():\n                  s3_path = os.path.normpath(\n                      os.path.join(os.path.join(subfolder, item.id, asset.href))\n                  )\n                  print(f\"upload {asset.href} to s3://{bucket}/{s3_path}\",file=sys.stderr)\n                  client.upload_file(\n                      asset.get_absolute_href(),\n                      bucket,\n                      s3_path,\n                  )\n                  asset.href = f\"s3://{bucket}/{s3_path}\"\n                  item.add_asset(key, asset)\n\n          cat.normalize_hrefs(f\"s3://{bucket}/{subfolder}\")\n\n          for item in cat.get_items():\n              # upload item to S3\n              print(f\"upload {item.id} to s3://{bucket}/{subfolder}\", file=sys.stderr)\n              pystac.write_file(item, item.get_self_href())\n\n          # upload catalog to S3\n          print(f\"upload catalog.json to s3://{bucket}/{subfolder}\", file=sys.stderr)\n          pystac.write_file(cat, cat.get_self_href())\n\n          print(f\"s3://{bucket}/{subfolder}/catalog.json\", file=sys.stdout)\n</code></pre> <p>The Python script <code>stage.py</code> uploads a Spatio-Temporal Asset Catalog (STAC) catalog to an Object Storage S3 bucket.  This script utilizes the <code>boto3</code> library for the S3 operations and customizes the STAC I/O to read from and write to an S3 bucket.</p> <p>Here's a breakdown of what the script does:</p> <ul> <li> <p>The script imports necessary libraries and modules, including <code>os</code>, <code>sys</code>, <code>pystac</code>, <code>botocore</code>, <code>boto3</code>, <code>shutil</code>, and more.</p> </li> <li> <p>It reads command-line arguments to get the STAC catalog URL, S3 bucket name, and a subfolder path to where the catalog will be uploaded.</p> </li> <li> <p>It retrieves the S3 credentials and endpoint URL from environment variables.</p> </li> <li> <p>The script copies the STAC catalog (from the given URL) to a temporary directory under \"/tmp/catalog.\"</p> </li> <li> <p>A custom STAC I/O class (<code>CustomStacIO</code>) is defined. This class extends the <code>DefaultStacIO</code> class and uses <code>boto3</code> to interact with S3. It has methods for writing STAC text content to S3.</p> </li> <li> <p>A connection to S3 is established using <code>boto3</code> with the provided S3 credentials and endpoint URL.</p> </li> <li> <p>The default STAC I/O class is set to the custom <code>CustomStacIO</code>.</p> </li> <li> <p>The script iterates through items in the STAC catalog, including their assets. For each asset, it uploads the data to the specified S3 bucket under the given subfolder and updates the asset's href to the S3 location.</p> </li> <li> <p>After uploading all assets, it normalizes the catalog's hrefs to point to the S3 location.</p> </li> <li> <p>The script then uploads each item in the catalog to S3, updating their hrefs accordingly.</p> </li> <li> <p>Finally, it uploads the catalog itself to S3.</p> </li> <li> <p>The script prints the S3 URL of the uploaded catalog to the standard output.</p> </li> </ul> <p>This script uploads a STAC catalog and its associated assets to an S3 bucket, making it accessible through the S3 endpoint. </p> <p>It uses a custom STAC I/O to facilitate S3 interactions.</p> <p>The AWS credentials and endpoint URL need to be properly configured in the environment variables for the script to work. A</p> <pre><code>export WORKSPACE=/workspace/mastering-app-package\n\npodman \\\n    build \\\n    --format docker \\\n    -t localhost/stage:latest \\\n    ${WORKSPACE}/water-bodies/command-line-tools/stage\n</code></pre> <p>To run the stage-out step, one would run:</p> <pre><code>cwltool \\\n    --podman \\\n    ${WORKSPACE}/cwl-cli/stage-out.cwl \\\n    --aws_access_key_id $( cat ~/.aws/credentials | grep aws_access_key_id | cut -d \"=\" -f 2 ) \\\n    --aws_secret_access_key $( cat ~/.aws/credentials | grep aws_secret_access_key | cut -d \"=\" -f 2 ) \\\n    --endpoint_url $( cat ~/.aws/config | grep endpoint_url | head -n 1 | cut -d \"=\" -f 2 ) \\\n    --region_name $( cat ~/.aws/config | grep region | cut -d \"=\" -f 2 ) \\\n    --s3_bucket processing-results \\\n    --sub_path processing-results/$( cat /proc/sys/kernel/random/uuid ) \\\n    --stac_catalog ${WORKSPACE}/runs\n</code></pre>"},{"location":"cwl-workflow/staged/","title":"Processing a staged Landsat-9 acquisition","text":"<p>The non-Cloud native Workflow chains the <code>crop</code>, <code>norm_diff</code>, <code>otsu</code> and <code>stac</code> steps taking a staged EO acquisition as a STAC Catalog in a directory as input parameters:</p> <ul> <li>a local STAC Catalog in a directory</li> <li>a bounding box area of interest (AOI)</li> <li>the EPSG code of the bounding box area of interest</li> <li>a list of common band names ([\"green\", \"nir08\"])</li> </ul> graph TB S[(local storage)] S -- STAC Catalog --&gt; B((\"crop(green)\")); S --  STAC Catalog --&gt; C((\"crop(nir)\")); S -- STAC Catalog --&gt; F P[bands] Q[EPSG code] R[AOI] subgraph scatter on bands   P --&gt; B((\"crop(green)\"))   P --&gt; C((\"crop(nir08)\"))   Q --&gt; B((\"crop(green)\"))   Q --&gt; C((\"crop(nir08)\"))   R --&gt; B((\"crop(green)\"))   R --&gt; C((\"crop(nir08)\")) end B((\"crop(green)\")) --&gt; D C((\"crop(nir08)\")) --&gt; D D((\"`Normalized  difference`\")); D --&gt; E((\"`Otsu  threshold`\")) E --&gt; F((\"`Create  STAC Catalog`\")) <p>The CWL Workflow is shown below and the lines highlighted show the changes to manage a staged STAC Catalog instead of an URL to a STAC Item:</p> app-water-body.cwl<pre><code>cwlVersion: v1.0\n$namespaces:\n  s: https://schema.org/\ns:softwareVersion: 1.0.0\nschemas:\n  - http://schema.org/version/9.0/schemaorg-current-http.rdf\n$graph:\n  - class: Workflow\n    id: water-bodies\n    label: Water body detection based on NDWI and the otsu threshold\n    doc: Water bodies detection based on NDWI and otsu threshold applied to Sentinel-2 or Landsat-9 staged acquisitions\n    requirements:\n      - class: ScatterFeatureRequirement\n    inputs:\n      aoi:\n        label: area of interest\n        doc: area of interest as a bounding box\n        type: string\n      epsg:\n        label: EPSG code\n        doc: EPSG code\n        type: string\n        default: \"EPSG:4326\"\n      bands:\n        label: bands used for the NDWI\n        doc: bands used for the NDWI\n        type: string[]\n        default: [\"green\", \"nir\"]\n      item:\n        doc: Reference to a STAC item\n        label: STAC item reference\n        type: Directory\n    outputs:\n      - id: stac_catalog\n        outputSource:\n          - node_stac/stac_catalog\n        type: Directory\n    steps:\n      node_crop:\n        run: \"#crop\"\n        in:\n          item: item\n          aoi: aoi\n          epsg: epsg\n          band: bands\n        out:\n          - cropped\n        scatter: band\n        scatterMethod: dotproduct\n      node_normalized_difference:\n        run: \"#norm_diff\"\n        in:\n          rasters:\n            source: node_crop/cropped\n        out:\n          - ndwi\n      node_otsu:\n        run: \"#otsu\"\n        in:\n          raster:\n            source: node_normalized_difference/ndwi\n        out:\n          - binary_mask_item\n      node_stac:\n        run: \"#stac\"\n        in:\n          item: item\n          rasters:\n            source: node_otsu/binary_mask_item\n        out:\n          - stac_catalog\n  - class: CommandLineTool\n    id: crop\n    requirements:\n      InlineJavascriptRequirement: {}\n      EnvVarRequirement:\n        envDef:\n          PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n          PYTHONPATH: /app\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 512\n    hints:\n      DockerRequirement:\n        dockerPull: localhost/crop:latest\n    baseCommand: [\"python\", \"-m\", \"app\"]\n    arguments: []\n    inputs:\n      item:\n        type: Directory\n        inputBinding:\n          prefix: --input-item\n      aoi:\n        type: string\n        inputBinding:\n          prefix: --aoi\n      epsg:\n        type: string\n        inputBinding:\n          prefix: --epsg\n      band:\n        type: string\n        inputBinding:\n          prefix: --band\n    outputs:\n      cropped:\n        outputBinding:\n          glob: '*.tif'\n        type: File\n  - class: CommandLineTool\n    id: norm_diff\n    requirements:\n      InlineJavascriptRequirement: {}\n      EnvVarRequirement:\n        envDef:\n          PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n          PYTHONPATH: /app\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 512\n    hints:\n      DockerRequirement:\n        dockerPull: localhost/norm-diff:latest\n    baseCommand: [\"python\", \"-m\", \"app\"]\n    arguments: []\n    inputs:\n      rasters:\n        type: File[]\n        inputBinding:\n          position: 1\n    outputs:\n      ndwi:\n        outputBinding:\n          glob: '*.tif'\n        type: File\n  - class: CommandLineTool\n    id: otsu\n    requirements:\n      InlineJavascriptRequirement: {}\n      EnvVarRequirement:\n        envDef:\n          PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n          PYTHONPATH: /app\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 512\n    hints:\n      DockerRequirement:\n        dockerPull: localhost/otsu:latest\n    baseCommand: [\"python\", \"-m\", \"app\"]\n    arguments: []\n    inputs:\n      raster:\n        type: File\n        inputBinding:\n          position: 1\n    outputs:\n      binary_mask_item:\n        outputBinding:\n          glob: '*.tif'\n        type: File\n  - class: CommandLineTool\n    id: stac\n    requirements:\n      InlineJavascriptRequirement: {}\n      EnvVarRequirement:\n        envDef:\n          PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n          PYTHONPATH: /app\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 512\n    hints:\n      DockerRequirement:\n        dockerPull: localhost/stac:latest\n    baseCommand: [\"python\", \"-m\", \"app\"]\n    arguments: []\n    inputs:\n      item:\n        type: Directory\n        inputBinding:\n          prefix: --input-item\n      rasters:\n        type: File\n        inputBinding:\n          prefix: --water-body\n    outputs:\n      stac_catalog:\n        outputBinding:\n          glob: .\n        type: Directory\n</code></pre>"},{"location":"exec/scope/","title":"Scope","text":"<p>When developers package and EO, they are in fact packaging their own software, written in a specific programming language, as a containerized application (or a set of containerized applications), to be described as an EO Application Package using the Common Workflow Language as described in the OGC proposed best practices.</p> <p>To achieve this, developers follow the steps described below.</p> <ul> <li>Prepare one or more container images containing the execution dependencies of the software.</li> <li>Prepare the CWL CommandLineTool document(s) wrapping the command line tool available in container(s).</li> <li>Prepare the CWL Workflow orchestrating CWL CommandLineTool document(s) wrapping the command line tool available container(s).</li> <li>Test the application package in one or more execution scenarios.</li> </ul> <p>This page covers the step:</p> <p>Test the application package in one or more execution scenarios</p>"},{"location":"fair/best-practice/","title":"FAIR Application Packages","text":""},{"location":"fair/best-practice/#application-package-software-configuration-management","title":"Application Package Software Configuration Management","text":"<p>The SCM has the task of tracking and controlling changes in the software as a part of the larger cross-disciplinary field of configuration management. </p> <p>SCM practices include revision control and the establishment of baselines.</p> <p>The Application Package code is hosted on a repository publicly accessible (Github, Bitbucket, a GitLab instance, an institutional software forge, etc.) using one of the version control systems supported by (Subversion, Mercurial and Git)</p> <p>The Application Package code include, at the top level of the source code tree, the following files:</p> <ul> <li>README containing a description of the software (name, purpose, pointers to website, documentation, development platform, contact, and support information, \u2026)</li> <li>AUTHORS, a list of all the persons to be credited for the software.</li> <li>LICENSE, the project license terms. For Open Source Licenses, the standard SPDX license names are used. For large software projects and developers, the REUSE (https://reuse.software/) process and tools can be an option to look at.</li> <li>codemeta.json, a linked data metadata file that helps index the source code in the Software Heritage archive and provides an easy way to link to other related research outputs.</li> </ul> <p>The code meta project motivation (https://codemeta.github.io/) is reported below:</p> <p>Research relies heavily on scientific software, and a large and growing fraction of researchers are engaged in developing software as part of their own research (Hannay et al 2009). Despite this, infrastructure to support the preservation, discovery, reuse, and attribution of software lags substantially behind that of other research products such as journal articles and research data. This lag is driven not so much by a lack of technology as it is by a lack of unity: existing mechanisms to archive, document, index, share, discover, and cite software contributions are heterogeneous among both disciplines and archives and rarely meet best practices (Howison 2015). Fortunately, a rapidly growing movement to improve preservation, discovery, reuse, and attribution of academic software is now underway: a recent NIH report; conferences and working groups of FORCE11, WSSSPE &amp; Software Sustainability Institute; and the rising adoption of repositories like GitHub, Zenodo, figshare &amp; DataONE by academic software developers. Now is the time to improve how these resources can communicate to each other.</p> <p>CodeMeta developed the translations from the different vocabularies. The CodeMeta vocabulary is an extension of the SoftwareApplication and SoftwareSourceCode classes found in the vocabulary of the Schema.org initiative [schema]. Metadata information conformant to the CodeMeta vocabulary can be represented in JSON format, named codemeta.json, like the example below with the information for Water Bodies Detection application.</p> codemeta.json<pre><code>{\n    \"@context\": \"https://doi.org/10.5063/schema/codemeta-2.0\",\n    \"@type\": \"SoftwareSourceCode\",\n    \"license\": \"https://spdx.org/licenses/CC-BY-NC-SA-4.0\",\n    \"codeRepository\": \"https://github.com/eoap/mastering-app-package.git\",\n    \"dateCreated\": \"2022-09-01\",\n    \"datePublished\": \"2022-09-25\",\n    \"dateModified\": \"2024-11-07\",\n    \"name\": \"Water Bodies Detection\",\n    \"version\": \"1.1.0\",\n    \"description\": \"The Water Bodies Detection is an Application that uses the NDWI index and the Otsu threshold to detect water bodies using Sentinel-2 or Landsat-9 data\",\n    \"developmentStatus\": \"active\",\n    \"downloadUrl\": \"https://github.com/eoap/mastering-app-package/releases/tag/1.1.0\",\n    \"relatedLink\": [\n        \"https://eoap.github.io/mastering-app-package\"\n    ],\n    \"funder\": {\n        \"@type\": \"Organization\",\n        \"name\": \"Terradue\"\n    },\n    \"keywords\": [\n        \"NDWI\", \"Landsat-9\", \"Sentinel-2\", \"Water Bodies\"\n    ],\n    \"programmingLanguage\": [\n        \"Python\", \"CWL\"\n    ],\n    \"softwareRequirements\": [\n        \"container runtime\",\n        \"cwl runner\"\n    ],\n    \"author\": [\n        {\n            \"@type\": \"Person\",\n            \"givenName\": \"Jane\",\n            \"familyName\": \"Doe\",\n            \"email\": \"jane.doe@acme.earth\",\n            \"affiliation\": {\n                \"@type\": \"Organization\",\n                \"name\": \"ACME\"\n            }\n        },\n        {\n            \"@type\": \"Person\",\n            \"givenName\": \"John\",\n            \"familyName\": \"Doe\",\n            \"email\": \"john.doe@acme.earth\",\n            \"affiliation\": {\n                \"@type\": \"Organization\",\n                \"name\": \"ACME\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"fair/best-practice/#application-package-software-identification-and-traceability","title":"Application Package Software identification and traceability","text":"<p>In a Reproducible FAIR Workflow scenario, ensuring that the code behind the application is uniquely identified and traceable is necessary. Even if software configuration management systems have become a common tool for tracking and controlling changes in the software, the FAIR publication of applications also needs methods for software citation, software retrieval, and long-term preservation. This is achieved with the issuing of a persistent identifier.</p> <p>The persistent identifier provides a long-lasting reference to source code and is usually understood as an actionable and accessible reference over the Internet that retrieves the necessary files. As such, the persistent component is dependent on the service commitment to resolve the identifier and the dedicated storage lifespan.</p> <p>Several initiatives provide this persistent dual functionality of identification and archivation. This Best Practice identifies one solution with Software Heritage. Software Heritage is a non-profit multi-stakeholder initiative unveiled in 2016 by the French Institute for Research in Computer Science and Automation (Inria) and supported by UNESCO. The mission of Software Heritage is to collect, preserve, and share all software that is publicly available in source code form, with the goal of building a common shared infrastructure at the service of industry, research, culture, and society.</p> <p>The Software Heritage provides SoftWare Heritage persistent IDentifiers (SWHIDs) and ensures preservation of the software source code by crawling code hosting platforms, like GitHub, GitLab.com, or Bitbucket, and package archives, like npm or PyPI, ingested into a special data structure, a Merkle DAG, that is the core of the archive.</p> <p>To obtain a SWHID, the code must be hosted on a publicly accessible repository (Github, Bitbucket, a GitLab instance, an institutional software forge, etc.) using one of the version control systems supported by Software Heritage (Subversion, Mercurial, and Git).</p>"},{"location":"fair/best-practice/#application-package-containers-identification","title":"Application Package Containers identification","text":"<p>A Reproducible FAIR Workflow scenario also requires that the application container(s) is(are) uniquely identified and traceable. </p> <p>Using the sha256 signature of the container, containers have an identifier.</p> <p>Example:</p> <pre><code>- class: CommandLineTool\n  id: crop\n\n  hints:\n    DockerRequirement:\n      dockerPull: docker.io/terradue/crop@sha256:ec2d8e71ab5834cb9db01c5001bde9c3d6038d0418ad085726b051b4359750e1\n</code></pre>"},{"location":"fair/best-practice/#application-package-metadata","title":"Application Package metadata","text":"<p>The OGC Best Practices for EO Application Packages (OGC 20-089) recommends that to enrich the application package with new concepts, these should originate from schema.org and be linked with their RDF encoding. While OGC 20-089 only enforces the version element as a mandatory, it already suggest several additional elements that are key for the Reproducible FAIR Workflows scenario such as the following.</p> <ul> <li>author: The main author of the Application Package - https://schema.org/author.</li> <li>citation: A citation or reference to a publication, web page, scholarly article, etc. https://schema.org/citation.</li> <li>codeRepository: Link to the repository where the Application code is located (e.g., SVN, github). https://schema.org/codeRepository.</li> <li>contributor: A secondary contributor to the Application Package https://schema.org/contributor.</li> <li>dateCreated: The date on which the Application Package was created. https://schema.org/dateCreated.</li> <li>keywords: Keywords used to describe this application. Multiple entries in a keywords list are delimited by commas. https://schema.org/keywords.</li> <li>license: An URL to the license document that applies to this application. https://schema.org/license.</li> <li>releaseNotes: Description of what changed in this version. https://schema.org/releaseNotes.</li> </ul> <p>All these elements can be obtained from the CodeMeta vocabulary (i.e., codemeta.json) and can be directly added when defining the Application Package (e.g. during the Continuous Integration).</p> <p>Example:</p> <pre><code>cwlVersion: v1.0\n$graph:\n...\n$namespaces:\n  s: https://schema.org/\ns:author:\n- class: s:Person\n  s:affiliation: Planet Earth\n  s:email: john.doe@somedomain.org\n  s:name: Doe, John\ns:contributor:\n- class: s:Person\n  s:affiliation: Planet Earth\n  s:email: jane.doe@somedomain.org\n  s:name: Doe, Jane\ns:softwareVersion: 1.1.6\nschemas:\n- http://schema.org/version/9.0/schemaorg-current-http.rdf\n</code></pre>"},{"location":"fair/best-practice/#application-package-identification-and-traceability","title":"Application Package identification and traceability","text":"<p>In a Reproducible FAIR Workflow scenario, it is necessary to ensure that the Application Package is uniquely identified and traceable. </p> <p>The assignment of a DOI to the application package extends the metadata section of the Application Package CWL with:</p> <pre><code>sameAs: URL of a reference Web page that unambiguously indicates the item\u2019s identity. https://schema.org/sameAs.\n</code></pre> <p>Example</p> <pre><code>cwlVersion: v1.0\n$graph:\n...\n$namespaces:\n  s: https://schema.org/\ns:author:\n- class: s:Person\n  s:affiliation: Planet Earth\n  s:email: john.doe@somedomain.org\n  s:name: Doe, John\ns:contributor:\n- class: s:Person\n  s:affiliation: Planet Earth\n  s:email: jane.doe@somedomain.org\n  s:name: Doe, Jane\ns:sameas: https://doi.org/10.5072/zenodo.1107209\ns:softwareVersion: 1.1.6\nschemas:\n- http://schema.org/version/9.0/schemaorg-current-http.rdf\n</code></pre>"},{"location":"kubernetes/calrissian/","title":"Run the CWL Workflow with calrissian","text":""},{"location":"kubernetes/calrissian/#goal","title":"Goal","text":"<p>Run the <code>app-water-body-cloud-native.1.0.0.cwl</code> released application package using <code>calrissian</code>, a CWL runner for kubernetes.</p> <p><code>calrissian</code> creates a pod for each of the Workflow processing step.</p>"},{"location":"kubernetes/calrissian/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/Kubernetes/calrissian.ipynb</p>"},{"location":"kubernetes/calrissian/#step-1-configure-the-workspace","title":"Step 1 - Configure the workspace","text":"<p>The results produced will be available in the local folder <code>/workspace/mastering-app-package/runs</code></p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n</code></pre>"},{"location":"kubernetes/calrissian/#step-2-download-the-released-application-package","title":"Step 2 - Download the released Application package","text":"<pre><code>version=\"1.0.0\"\n\nwget \\\n    -O ${WORKSPACE}/runs/app-water-bodies-cloud-native.${version}.cwl \\\n    https://github.com/eoap/mastering-app-package/releases/download/${version}/app-water-bodies-cloud-native.${version}.cwl\n</code></pre> <pre><code>sh ${WORKSPACE}/scripts/download-app-water-bodies-cloud-native.sh\n</code></pre>"},{"location":"kubernetes/calrissian/#step-3-execute-the-application-package","title":"Step 3 - Execute the Application Package","text":"<pre><code>mkdir -p /calrissian/logs\n\nversion=\"1.0.0\"\n\ncalrissian \\\n    --stdout /calrissian/results.json \\\n    --stderr /calrissian/app.log \\\n    --max-ram 4G \\\n    --max-cores \"8\" \\\n    --tmp-outdir-prefix /calrissian/tmp \\\n    --outdir /calrissian/results \\\n    --usage-report /calrissian/usage.json \\\n    --tool-logs-basepath /calrissian/logs \\\n    --pod-nodeselectors /etc/calrissian/pod-node-selector.yaml \\\n    /workspace/mastering-app-package/runs/app-water-bodies-cloud-native.${version}.cwl#water-bodies \\\n    --stac_items \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2A_10TFK_20210708_0_L2A\" \\\n    --stac_items \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_10TFK_20210713_0_L2A\" \\\n    --stac_items \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2A_10TFK_20210718_0_L2A\" \\\n    --stac_items \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2A_10TFK_20220524_0_L2A\" \\\n    --stac_items \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2A_10TFK_20220514_0_L2A\" \\\n    --stac_items \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2A_10TFK_20220504_0_L2A\" \\\n    --aoi=\"-121.399,39.834,-120.74,40.472\" \\\n    --epsg \"EPSG:4326\"\n</code></pre> <pre><code>sh ${WORKSPACE}/scripts/calrissian-cloud-native.sh\n</code></pre> <p>New pods are created:</p> <pre><code>(base) jovyan@coder-mrossi:~/mastering-app-package$ kubectl get pods\nNAME                       READY   STATUS              RESTARTS   AGE\ncoder-mrossi               1/1     Running             0          1h\nnode-crop-2-pod-laqxbhhy   0/1     ContainerCreating   0          8s\nnode-crop-3-pod-gvdnujzn   0/1     ContainerCreating   0          8s\nnode-crop-4-pod-qisbfayc   0/1     ContainerCreating   0          8s\nnode-crop-5-pod-joficzgy   0/1     ContainerCreating   0          8s\nnode-crop-6-pod-ejarwpxk   0/1     ContainerCreating   0          8s\nnode-crop-7-pod-ecjgjglq   0/1     ContainerCreating   0          8s\nnode-crop-pod-ebgkkvpc     0/1     ContainerCreating   0          8s\n</code></pre>"},{"location":"kubernetes/calrissian/#step-4-inspect-the-results","title":"Step 4 - Inspect the results","text":"<pre><code>tree $( cat /calrissian/results.json | jq -r .stac_catalog.path )\n</code></pre>"},{"location":"kubernetes/calrissian/#expected-outcome","title":"Expected outcome","text":"<pre><code>(base) jovyan@coder-mrossi:~/runs$ tree $( cat /calrissian/results.json | jq -r .stac_catalog.path )\n/calrissian/results/tmp3vhr5k8r\n\u251c\u2500\u2500 S2A_10TFK_20210708_0_L2A\n\u2502   \u251c\u2500\u2500 S2A_10TFK_20210708_0_L2A.json\n\u2502   \u2514\u2500\u2500 otsu.tif\n\u251c\u2500\u2500 S2A_10TFK_20210718_0_L2A\n\u2502   \u251c\u2500\u2500 S2A_10TFK_20210718_0_L2A.json\n\u2502   \u2514\u2500\u2500 otsu.tif\n\u251c\u2500\u2500 S2A_10TFK_20220504_0_L2A\n\u2502   \u251c\u2500\u2500 S2A_10TFK_20220504_0_L2A.json\n\u2502   \u2514\u2500\u2500 otsu.tif\n\u251c\u2500\u2500 S2A_10TFK_20220514_0_L2A\n\u2502   \u251c\u2500\u2500 S2A_10TFK_20220514_0_L2A.json\n\u2502   \u2514\u2500\u2500 otsu.tif\n\u251c\u2500\u2500 S2A_10TFK_20220524_0_L2A\n\u2502   \u251c\u2500\u2500 S2A_10TFK_20220524_0_L2A.json\n\u2502   \u2514\u2500\u2500 otsu.tif\n\u251c\u2500\u2500 S2B_10TFK_20210713_0_L2A\n\u2502   \u251c\u2500\u2500 S2B_10TFK_20210713_0_L2A.json\n\u2502   \u2514\u2500\u2500 otsu.tif\n\u2514\u2500\u2500 catalog.json\n\n6 directories, 13 files\n</code></pre>"},{"location":"prerequisites/cwl/","title":"Introduction to CWL","text":"<p>The paper Methods Included: Standardizing Computational Reuse and Portability with the Common Workflow Language provides an excellent description of the Common Workflow Language project producing free and open standards for describing command-line tool based workflows.</p>"},{"location":"prerequisites/cwl/#tldr","title":"TL;DR","text":"<p>Although the paper provides a clear and concise description of the CWL standards, here's a light summary wrapping up the main points to provide the required concepts behind this guide.</p>"},{"location":"prerequisites/cwl/#cwl-key-insights","title":"CWL Key Insights","text":"<ol> <li> <p>CWL is a set of standards for describing and sharing computational workflows.</p> </li> <li> <p>The CWL standards are used daily in many science and engineering domains, including by multi-stakeholder teams.</p> </li> <li> <p>The CWL standards use a declarative syntax, facilitating polylingual workflow tasks. By being explicit about the run-time environment and any use of software containers, the CWL standards enable portability and reuse. </p> </li> <li> <p>The CWL standards provide a separation of concerns between workflow authors and workflow platforms.</p> </li> <li> <p>The CWL standards support critical workflow concepts like automation, scalability, abstraction, provenance, portability, and reusability. </p> </li> <li> <p>The CWL standards are developed around core principles of community and shared decision-making, re-use, and zero cost for participants.</p> </li> <li> <p>The CWL standards are provided as freely available open standards, supported by a diverse community in collaboration with industry, and is a Free/Open Source Software ecosystem </p> </li> </ol>"},{"location":"prerequisites/cwl/#cwl-features","title":"CWL Features","text":"<p>The CWL standard support polylingual and multi-party workflows and includes two main components:</p> <ol> <li> <p>A standard for describing command line tools</p> </li> <li> <p>A standard for describing workflows that compose such tool descriptions</p> </li> </ol> <p>The CWL standards define an explicit language with a textual syntax derived from YAML</p>"},{"location":"prerequisites/cwl/#cwl-command-line-tool-description-standard","title":"CWL Command Line Tool Description Standard","text":"<p>The CWL Command Line Tool Description Standard describes:</p> <ul> <li>how a particular command line tool works: what are the inputs and parameters and their types</li> <li>how to add the correct flags and switches to the command line invocation </li> <li>where to find the output files</li> </ul>"},{"location":"prerequisites/cwl/#cwl-workflow-description-standard","title":"CWL Workflow Description Standard","text":"<p>The CWL Workflow Description Standard is based on the same textual syntax derived from YAML to explicit workflow level inputs, outputs and steps.</p> <p>Steps are comprised of CWL CommandLineTools or CWL sub-workflows, each re-exposing their tool\u2019s required inputs. </p> <p>Inputs for each step are connected by referencing the name of either the common workflow inputs or particular outputs of other steps.</p> <p>The workflow outputs expose selected outputs from workflow steps.</p> <p>Being CWL a set of standards, the workflows are executed using a CWL runner and there are several implementations of such runners. </p> <p>This guide uses the CWL runner cwltool.</p>"},{"location":"prerequisites/cwl/#recommendations","title":"Recommendations","text":"<ul> <li> <p>Include documentation and labels for all components to enable the automatic generation of helpful visual depictions for any given CWL description</p> </li> <li> <p>Include metadata about the tool  </p> </li> <li> <p>Include a Workflow class for all CommandLineTools (a single step Workflow)</p> </li> <li> <p>Organize your CWL files is several individual files to ease their readability and maintenance. Pack your multi-file CWL Workflows (<code>cwltool --pack</code>) when needed</p> </li> </ul>"},{"location":"prerequisites/cwl/#references","title":"References","text":"<ul> <li>Crusoe, M. R. et al. Methods Included: Standardizing Computational Reuse and Portability with the Common Workflow Language, retrieved from https://arxiv.org/abs/2105.07028</li> </ul>"},{"location":"prerequisites/yaml/","title":"Introduction to YAML","text":""},{"location":"prerequisites/yaml/#key-value-pairs","title":"Key-Value Pairs","text":"<p>Fundamentally, a file written in YAML consists of a set of key-value pairs.</p> <p>Each pair is written as <code>key: value</code>, where whitespace after the <code>:</code> is optional.</p> <p>Key names in CWL files should not contain whitespace - We use camelCase for multi-word key names that have special meaning in the CWL specification and underscored key names otherwise.</p> <p>For example:</p> <pre><code>first_name: Bilbo\nlast_name:  Baggins\nage_years:  111\nhome:       Bag End, Hobbiton\n</code></pre> <p>The YAML above defines four keys - <code>first_name</code>, <code>last_name</code>, <code>age_years</code>, and <code>home</code> - with their four respective values.</p> <p>Values can be:</p> <ul> <li>character strings</li> <li>numeric (integer, floating point, or scientific representation)</li> <li>Boolean (<code>true</code> or <code>false</code>)</li> </ul> <p>or more complex nested types (see below).</p> <p>Values may be wrapped in quotation marks but be aware that this may change the way that they are interpreted i.e. <code>\"1234\"</code> will be treated as a character string, while <code>1234</code> will be treated as an integer.</p> <p>This distinction can be important,for example when describing parameters to a command. In CWL all parts of <code>baseCommand</code> must be strings so, if you want to specify a fixed numeric value to a command,make sure that you wrap that numeric value in quotes: <code>baseCommand: [echo, \"42\"]</code>.</p>"},{"location":"prerequisites/yaml/#comments","title":"Comments","text":"<p>You may use <code>#</code> to add comments to your CWL and parameter files.</p> <p>Any characters to the right of <code>#</code> will be ignored by the program interpreting the YAML.</p> <p>For example:</p> <pre><code>first_name: Bilbo\nlast_name:  Baggins\nage_years:  111\n# this line will be ignored by the interpreter\nhome:       Bag End, Hobbiton # this is ignored too\n</code></pre> <p>If there is anything on the line before the comment, be sure to add at least one space before the <code>#</code>!</p>"},{"location":"prerequisites/yaml/#maps","title":"Maps","text":"<p>When describing a tool or workflow with CWL, it is usually necessary to construct more complex, nested representations.</p> <p>Called maps, these hierarchical structures are described in YAML by providing additional key-value pairs as the value of any key.</p> <p>These pairs (sometimes referred to as \"children\") are written on new lines under the key to which they belong (the \"parent\"), and should be indented with two spaces (\u21e5tab characters are not allowed).</p> <p>For example:</p> <pre><code>cwlVersion: v1.0\nclass: CommandLineTool\nbaseCommand: echo\ninputs: # this key has an object value\n  example_flag: # so does this one\n    type: boolean\n    inputBinding: # and this one too\n      position: 1\n      prefix: -f\n</code></pre> <p>The YAML above illustrates how you can build up complex nested object descriptions relatively quickly.</p> <p>The <code>inputs</code> map contains a single key, <code>example_flag</code>, which itself contains two keys, <code>type</code> and <code>inputBinding</code>, while one of these children, <code>inputBinding</code>, contains a further two key-value pairs (<code>position</code> and <code>prefix</code>).</p> <p>See the Arrays section below for more information about providing multiple values/key-value pairs for a single key.</p> <p>For comparison with the example YAML above, here is a graphical representation of the <code>inputs</code> object it describes.</p>  graph TD   inputs --&gt; example_flag   example_flag --&gt; type   type --- bool((boolean))   example_flag --&gt; inputBinding   inputBinding --&gt; position   inputBinding --&gt; prefix   position --- posval((1))   prefix --- prefval(('-f'))"},{"location":"prerequisites/yaml/#arrays","title":"Arrays","text":"<p>In certain circumstances it is necessary to provide multiple values or objects for a single key.</p> <p>As we've already seen in the Maps section above, more than one key-value pair can be mapped to a single key.</p> <p>However, it is also possible to define multiple values for a key without having to provide a unique key for each value.</p> <p>We can achieve this with an array, where each value is defined on its own line and preceded by <code>-</code>.</p> <p>For example:</p> <pre><code>touchfiles:\n  - foo.txt\n  - bar.dat\n  - baz.txt\n</code></pre> <p>and a more complex example combining maps and arrays:</p> <pre><code>exclusive_parameters:\n  type:\n    - type: record\n      name: itemC\n      fields:\n        itemC:\n          type: string\n          inputBinding:\n            prefix: -C\n    - type: record\n      name: itemD\n      fields:\n        itemD:\n          type: string\n          inputBinding:\n            prefix: -D\n</code></pre>"},{"location":"prerequisites/yaml/#json-style","title":"JSON Style","text":"<p>YAML is based on JavaScript Object Notation (JSON) and maps and arrays can also be defined in YAML using the native JSON syntax.</p> <p>For example:</p> <pre><code>touchfiles: [foo.txt, bar.dat, baz.txt] # equivalent to first Arrays example\n</code></pre> <p>and:</p> <pre><code># equivalent to the `inputs` example in \"Maps\" above\ninputs: {example_flag: {type: boolean, inputBinding: {position: 1, prefix: -f}}}\n</code></pre> <p>Native JSON can be useful to indicate where a field is being left intentionally empty (such as <code>[]</code> for an empty array), and where it makes more sense for the values to be located on the same line (such as when providing option flags and their values in a shell command).</p> <p>However, as the second example above shows, it can severely affect the readability of a YAML file and should be used sparingly.</p>"},{"location":"prerequisites/yaml/#reference","title":"Reference","text":"<p>This page is the same as http://www.commonwl.org/user_guide/yaml/</p>"},{"location":"python-env/crop/","title":"Running the step","text":""},{"location":"python-env/crop/#goal","title":"Goal","text":"<p>Run the <code>crop</code> step in a Python virtual environment.</p>"},{"location":"python-env/crop/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/Application steps/crop.ipynb</p>"},{"location":"python-env/crop/#step-1-configure-the-workspace","title":"Step 1 - Configure the workspace","text":"<p>The results produced will be available in the local folder <code>/workspace/mastering-app-package/runs</code></p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n</code></pre> <pre><code>source /workspace/mastering-app-package/scripts/setup.sh\n</code></pre>"},{"location":"python-env/crop/#step-2-create-the-python-virtual-environment","title":"Step 2 - Create the Python virtual environment","text":"<p>The required Python modules are installed using <code>pip</code>:</p> terminal<pre><code>python -m venv env_crop\nsource env_crop/bin/activate\npip install --no-cache-dir rasterio click pystac loguru pyproj shapely \n</code></pre> <pre><code>source ${WORKSPACE}/scripts/crop_env.sh\n</code></pre>"},{"location":"python-env/crop/#step-3-crop-the-green-band","title":"Step 3 - Crop the green band","text":"<p>The command line tool is invoked to crop the green band:</p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=/workspace/mastering-app-package/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n\npython \\\n    ${WORKSPACE}/water-bodies/command-line-tools/crop/app.py \\\n    --input-item \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_10TFK_20210713_0_L2A\" \\\n    --aoi \"-121.399,39.834,-120.74,40.472\" \\\n    --epsg \"EPSG:4326\" \\\n    --band green \n</code></pre> <pre><code>source ${WORKSPACE}/scripts/crop_green.sh\n</code></pre>"},{"location":"python-env/crop/#step-4-crop-the-nir-band","title":"Step 4 - Crop the NIR band","text":"<p>The command line tool is invoked to crop the NIR band:</p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n\npython \\\n    ${WORKSPACE}/water-bodies/command-line-tools/crop/app.py \\\n    --input-item \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_10TFK_20210713_0_L2A\" \\\n    --aoi \"-121.399,39.834,-120.74,40.472\" \\\n    --epsg \"EPSG:4326\" \\\n    --band nir \n</code></pre> <pre><code>source ${WORKSPACE}/scripts/crop_nir.sh\n</code></pre>"},{"location":"python-env/crop/#step-5-clean-up","title":"Step 5 - Clean-up","text":"<p>The Python virtual environment is no longer needed.</p> terminal<pre><code>deactivate\nrm -fr env_crop\n</code></pre> <pre><code>source ${WORKSPACE}/scripts/crop_deactivate.sh\n</code></pre>"},{"location":"python-env/crop/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-mrossi:~/runs$ tree .\n.\n\u251c\u2500\u2500 crop_green.tif\n\u2514\u2500\u2500 crop_nir.tif\n\n0 directories, 2 files\n</code></pre>"},{"location":"python-env/norm-diff/","title":"Running the step","text":""},{"location":"python-env/norm-diff/#goal","title":"Goal","text":"<p>Run the <code>normalized-difference</code> step in a Python virtual environment.</p>"},{"location":"python-env/norm-diff/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/Application steps/normalized-difference.ipynb</p>"},{"location":"python-env/norm-diff/#step-1-configure-the-workspace","title":"Step 1 - Configure the workspace","text":"<p>The results produced will be available in the local folder <code>/workspace/mastering-app-package/runs</code></p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n</code></pre> <pre><code>source /workspace/mastering-app-package/scripts/setup.sh\n</code></pre>"},{"location":"python-env/norm-diff/#step-2-create-the-python-virtual-environment","title":"Step 2 - Create the Python virtual environment","text":"<p>The required Python modules are installed using <code>pip</code>:</p> terminal<pre><code>python -m venv env_norm_diff\nsource env_norm_diff/bin/activate\npip install --no-cache-dir rasterio click loguru\n</code></pre> <pre><code>source ${WORKSPACE}/scripts/norm_diff_env.sh\n</code></pre>"},{"location":"python-env/norm-diff/#step-3-calculate-the-normalized-difference","title":"Step 3 - Calculate the normalized difference","text":"<p>The command line tool is invoked to calculate the normalized difference between the green and NIR cropped bands:</p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n\npython \\\n    ${WORKSPACE}/water-bodies/command-line-tools/norm_diff/app.py \\\n    crop_green.tif \\\n    crop_nir.tif\n</code></pre> <pre><code>source ${WORKSPACE}/scripts/norm_diff.sh\n</code></pre>"},{"location":"python-env/norm-diff/#step-4-clean-up","title":"Step 4 - Clean-up","text":"<p>The Python virtual environment is no longer needed.</p> terminal<pre><code>deactivate\nrm -fr env_norm_diff\n</code></pre> <pre><code>source ${WORKSPACE}/scripts/norm_diff_deactivate.sh\n</code></pre>"},{"location":"python-env/norm-diff/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-mrossi:~/runs$ tree .\n.\n\u251c\u2500\u2500 crop_green.tif\n\u251c\u2500\u2500 crop_nir.tif\n\u2514\u2500\u2500 norm_diff.tif\n\n0 directories, 3 files\n</code></pre>"},{"location":"python-env/otsu/","title":"Running the step","text":""},{"location":"python-env/otsu/#goal","title":"Goal","text":"<p>Run the <code>otsu</code> step in a Python virtual environment.</p>"},{"location":"python-env/otsu/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/Application steps/otsu.ipynb</p>"},{"location":"python-env/otsu/#step-1-configure-the-workspace","title":"Step 1 - Configure the workspace","text":"<p>The results produced will be available in the local folder <code>/workspace/mastering-app-package/runs</code></p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n</code></pre> <pre><code>source /workspace/mastering-app-package/scripts/setup.sh\n</code></pre>"},{"location":"python-env/otsu/#step-2-create-the-python-virtual-environment","title":"Step 2 - Create the Python virtual environment","text":"<p>The required Python modules are installed using <code>pip</code>:</p> terminal<pre><code>python -m venv env_otsu\nsource env_otsu/bin/activate\npip install --no-cache-dir rasterio scikit-image click loguru\n</code></pre> <pre><code>source ${WORKSPACE}/scripts/otsu_env.sh\n</code></pre>"},{"location":"python-env/otsu/#step-3-calculate-the-ostu-threshold","title":"Step 3 - Calculate the ostu threshold","text":"<p>The command line tool is invoked to calculate  Otsu threshold against the normalized difference:</p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n\npython \\\n    ${WORKSPACE}/water-bodies/command-line-tools/otsu/app.py \\\n    norm_diff.tif\n</code></pre> <pre><code>source ${WORKSPACE}/scripts/otsu.sh\n</code></pre>"},{"location":"python-env/otsu/#step-4-clean-up","title":"Step 4 - Clean-up","text":"<p>The Python virtual environment is no longer needed.</p> terminal<pre><code>deactivate\nrm -fr env_otsu\n</code></pre> <pre><code>source ${WORKSPACE}/scripts/otsu_deactivate.sh\n</code></pre>"},{"location":"python-env/otsu/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-mrossi:~/runs$ tree .\n.\n\u251c\u2500\u2500 crop_green.tif\n\u251c\u2500\u2500 crop_nir.tif\n\u251c\u2500\u2500 norm_diff.tif\n\u2514\u2500\u2500 otsu.tif\n\n0 directories, 4 files\n</code></pre>"},{"location":"python-env/stac/","title":"Running the step","text":""},{"location":"python-env/stac/#goal","title":"Goal","text":"<p>Run the <code>stac</code> step in a Python virtual environment.</p>"},{"location":"python-env/stac/#lab","title":"Lab","text":"<p>This step has a dedicated lab available at /workspace/mastering-app-package/practice-labs/Application steps/stac.ipynb</p>"},{"location":"python-env/stac/#step-1-configure-the-workspace","title":"Step 1 - Configure the workspace","text":"<p>The results produced will be available in the local folder <code>/workspace/mastering-app-package/runs</code></p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n</code></pre> <pre><code>source /workspace/mastering-app-package/scripts/setup.sh\n</code></pre>"},{"location":"python-env/stac/#step-2-create-the-python-virtual-environment","title":"Step 2 - Create the Python virtual environment","text":"<p>The required Python modules are installed using <code>pip</code>:</p> terminal<pre><code>python -m venv env_stac\nsource env_stac/bin/activate\npip install --no-cache-dir pystac rio_stac loguru click\n</code></pre> <pre><code>source ${WORKSPACE}/scripts/stac_env.sh\n</code></pre>"},{"location":"python-env/stac/#step-3-generate-the-stac-catalog","title":"Step 3 - Generate the STAC Catalog","text":"<p>The command line tool is invoked to produce a STAC Catalog:</p> terminal<pre><code>export WORKSPACE=/workspace/mastering-app-package\nexport RUNTIME=${WORKSPACE}/runs\nmkdir -p ${RUNTIME}\ncd ${RUNTIME}\n\npython \\\n    ${WORKSPACE}/water-bodies/command-line-tools/stac/app.py \\\n    --input-item \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_10TFK_20210713_0_L2A\" \\\n    --water-body otsu.tif\n</code></pre> <pre><code>source ${WORKSPACE}/scripts/stac.sh\n</code></pre>"},{"location":"python-env/stac/#step-4-clean-up","title":"Step 4 - Clean-up","text":"<p>The Python virtual environment is no longer needed.</p> terminal<pre><code>deactivate\nrm -fr env_stac\n</code></pre> <pre><code>source ${WORKSPACE}/scripts/stac_deactivate.sh\n</code></pre>"},{"location":"python-env/stac/#expected-outcome","title":"Expected outcome","text":"<p>The folder <code>/workspace/mastering-app-package/runs</code> contains: </p> <pre><code>(base) jovyan@coder-mrossi:~/runs$ tree .\n.\n\u251c\u2500\u2500 S2B_10TFK_20210713_0_L2A\n\u2502   \u251c\u2500\u2500 S2B_10TFK_20210713_0_L2A.json\n\u2502   \u2514\u2500\u2500 otsu.tif\n\u251c\u2500\u2500 catalog.json\n\u251c\u2500\u2500 crop_green.tif\n\u251c\u2500\u2500 crop_nir.tif\n\u251c\u2500\u2500 norm_diff.tif\n\u2514\u2500\u2500 otsu.tif\n\n1 directory, 7 files\n</code></pre>"},{"location":"reference/calrissian/","title":"calrissian","text":"<pre><code>usage: calrissian [-h] [--basedir BASEDIR] [--outdir OUTDIR] [--log-dir LOG_DIR] [--parallel]\n                  [--preserve-environment ENVVAR | --preserve-entire-environment] [--rm-container | --leave-container] [--cidfile-dir CIDFILE_DIR]\n                  [--cidfile-prefix CIDFILE_PREFIX] [--tmpdir-prefix TMPDIR_PREFIX] [--tmp-outdir-prefix TMP_OUTDIR_PREFIX | --cachedir CACHEDIR]\n                  [--rm-tmpdir | --leave-tmpdir] [--move-outputs | --leave-outputs | --copy-outputs] [--enable-pull | --disable-pull]\n                  [--rdf-serializer RDF_SERIALIZER] [--eval-timeout EVAL_TIMEOUT] [--provenance PROVENANCE] [--enable-user-provenance]\n                  [--disable-user-provenance] [--enable-host-provenance] [--disable-host-provenance] [--orcid ORCID] [--full-name CWL_FULL_NAME]\n                  [--print-rdf | --print-dot | --print-pre | --print-deps | --print-input-deps | --pack | --version | --validate | --print-supported-versions | --print-subgraph | --print-targets | --make-template]\n                  [--strict | --non-strict] [--skip-schemas] [--no-doc-cache | --doc-cache] [--verbose | --quiet | --debug]\n                  [--write-summary WRITE_SUMMARY] [--strict-memory-limit] [--strict-cpu-limit] [--timestamps] [--js-console] [--disable-js-validation]\n                  [--js-hint-options-file JS_HINT_OPTIONS_FILE] [--user-space-docker-cmd CMD | --udocker | --singularity | --podman | --no-container]\n                  [--beta-dependency-resolvers-configuration BETA_DEPENDENCY_RESOLVERS_CONFIGURATION]\n                  [--beta-dependencies-directory BETA_DEPENDENCIES_DIRECTORY] [--beta-use-biocontainers] [--beta-conda-dependencies] [--tool-help]\n                  [--relative-deps {primary,cwd}] [--enable-dev] [--enable-ext] [--enable-color | --disable-color]\n                  [--default-container DEFAULT_CONTAINER] [--no-match-user] [--custom-net CUSTOM_NET]\n                  [--enable-ga4gh-tool-registry | --disable-ga4gh-tool-registry] [--add-ga4gh-tool-registry GA4GH_TOOL_REGISTRIES]\n                  [--on-error {stop,continue}] [--compute-checksum | --no-compute-checksum] [--relax-path-checks] [--force-docker-pull]\n                  [--no-read-only] [--overrides OVERRIDES] [--target TARGET | --single-step SINGLE_STEP | --single-process SINGLE_PROCESS]\n                  [--mpi-config-file MPI_CONFIG_FILE] [--max-ram MAX_RAM] [--max-cores MAX_CORES] [--pod-labels [POD_LABELS]]\n                  [--pod-env-vars [POD_ENV_VARS]] [--pod-nodeselectors [POD_NODESELECTORS]] [--pod-serviceaccount POD_SERVICEACCOUNT]\n                  [--usage-report [USAGE_REPORT]] [--stdout [STDOUT]] [--stderr [STDERR]] [--tool-logs-basepath [TOOL_LOGS_BASEPATH]] [--conf CONF]\n                  [cwl_document] ...\n\nReference executor for Common Workflow Language standards. Not for production use.\n\npositional arguments:\n  cwl_document          path or URL to a CWL Workflow, CommandLineTool, or ExpressionTool. If the `inputs_object` has a `cwl:tool` field indicating\n                        the path or URL to the cwl_document, then the `cwl_document` argument is optional.\n  inputs_object         path or URL to a YAML or JSON formatted description of the required input values for the given `cwl_document`.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --basedir BASEDIR\n  --outdir OUTDIR       Output directory. The default is the current directory.\n  --log-dir LOG_DIR     Log your tools stdout/stderr to this location outside of container This will only log stdout/stderr if you specify\n                        stdout/stderr in their respective fields or capture it as an output\n  --parallel            [experimental] Run jobs in parallel.\n  --preserve-environment ENVVAR\n                        Preserve specific environment variable when running CommandLineTools. May be provided multiple times. By default PATH is\n                        preserved when not running in a container.\n  --preserve-entire-environment\n                        Preserve all environment variables when running CommandLineTools without a software container.\n  --rm-container        Delete Docker container used by jobs after they exit (default)\n  --leave-container     Do not delete Docker container used by jobs after they exit\n  --tmpdir-prefix TMPDIR_PREFIX\n                        Path prefix for temporary directories. If --tmpdir-prefix is not provided, then the prefix for temporary directories is\n                        influenced by the value of the TMPDIR, TEMP, or TMP environment variables. Taking those into consideration, the current\n                        default is /tmp/.\n  --tmp-outdir-prefix TMP_OUTDIR_PREFIX\n                        Path prefix for intermediate output directories. Defaults to the value of --tmpdir-prefix.\n  --cachedir CACHEDIR   Directory to cache intermediate workflow outputs to avoid recomputing steps. Can be very helpful in the development and\n                        troubleshooting of CWL documents.\n  --rm-tmpdir           Delete intermediate temporary directories (default)\n  --leave-tmpdir        Do not delete intermediate temporary directories\n  --move-outputs        Move output files to the workflow output directory and delete intermediate output directories (default).\n  --leave-outputs       Leave output files in intermediate output directories.\n  --copy-outputs        Copy output files to the workflow output directory and don't delete intermediate output directories.\n  --enable-pull         Try to pull Docker images\n  --disable-pull        Do not try to pull Docker images\n  --rdf-serializer RDF_SERIALIZER\n                        Output RDF serialization format used by --print-rdf (one of turtle (default), n3, nt, xml)\n  --eval-timeout EVAL_TIMEOUT\n                        Time to wait for a Javascript expression to evaluate before giving an error, default 60s.\n  --print-rdf           Print corresponding RDF graph for workflow and exit\n  --print-dot           Print workflow visualization in graphviz format and exit\n  --print-pre           Print CWL document after preprocessing.\n  --print-deps          Print CWL document dependencies.\n  --print-input-deps    Print input object document dependencies.\n  --pack                Combine components into single document and print.\n  --version             Print version and exit\n  --validate            Validate CWL document only.\n  --print-supported-versions\n                        Print supported CWL specs.\n  --print-subgraph      Print workflow subgraph that will execute. Can combined with --target or --single-step\n  --print-targets       Print targets (output parameters)\n  --make-template       Generate a template input object\n  --strict              Strict validation (unrecognized or out of place fields are error)\n  --non-strict          Lenient validation (ignore unrecognized fields)\n  --skip-schemas        Skip loading of schemas\n  --no-doc-cache        Disable disk cache for documents loaded over HTTP\n  --doc-cache           Enable disk cache for documents loaded over HTTP\n  --verbose             Default logging\n  --quiet               Only print warnings and errors.\n  --debug               Print even more logging\n  --write-summary WRITE_SUMMARY, -w WRITE_SUMMARY\n                        Path to write the final output JSON object to. Default is stdout.\n  --strict-memory-limit\n                        When running with software containers and the Docker engine, pass either the calculated memory allocation from\n                        ResourceRequirements or the default of 1 gigabyte to Docker's --memory option.\n  --strict-cpu-limit    When running with software containers and the Docker engine, pass either the calculated cpu allocation from\n                        ResourceRequirements or the default of 1 core to Docker's --cpu option. Requires docker version &gt;= v1.13.\n  --timestamps          Add timestamps to the errors, warnings, and notifications.\n  --js-console          Enable javascript console output\n  --disable-js-validation\n                        Disable javascript validation.\n  --js-hint-options-file JS_HINT_OPTIONS_FILE\n                        File of options to pass to jshint. This includes the added option \"includewarnings\".\n  --user-space-docker-cmd CMD\n                        (Linux/OS X only) Specify the path to udocker. Implies --udocker\n  --udocker             (Linux/OS X only) Use the udocker runtime for running containers (equivalent to --user-space-docker-cmd=udocker).\n  --singularity         [experimental] Use Singularity runtime for running containers. Requires Singularity v2.6.1+ and Linux with kernel version\n                        v3.18+ or with overlayfs support backported.\n  --podman              [experimental] Use Podman runtime for running containers.\n  --no-container        Do not execute jobs in a Docker container, even when `DockerRequirement` is specified under `hints`.\n  --beta-dependency-resolvers-configuration BETA_DEPENDENCY_RESOLVERS_CONFIGURATION\n                        Dependency resolver configuration file describing how to adapt 'SoftwareRequirement' packages to current system.\n  --beta-dependencies-directory BETA_DEPENDENCIES_DIRECTORY\n                        Default root directory used by dependency resolvers configuration.\n  --beta-use-biocontainers\n                        Use biocontainers for tools without an explicitly annotated Docker container.\n  --beta-conda-dependencies\n                        Short cut to use Conda to resolve 'SoftwareRequirement' packages.\n  --tool-help           Print command line help for tool\n  --relative-deps {primary,cwd}\n                        When using --print-deps, print paths relative to primary file or current working directory.\n  --enable-dev          Enable loading and running unofficial development versions of the CWL standards.\n  --enable-ext          Enable loading and running 'cwltool:' extensions to the CWL standards.\n  --enable-color        Enable logging color (default enabled)\n  --disable-color       Disable colored logging (default false)\n  --default-container DEFAULT_CONTAINER\n                        Specify a default software container to use for any CommandLineTool without a DockerRequirement.\n  --no-match-user       Disable passing the current uid to `docker run --user`\n  --custom-net CUSTOM_NET\n                        Passed to `docker run` as the '--net' parameter when NetworkAccess is true, which is its default setting.\n  --enable-ga4gh-tool-registry\n                        Enable tool resolution using GA4GH tool registry API\n  --disable-ga4gh-tool-registry\n                        Disable tool resolution using GA4GH tool registry API\n  --add-ga4gh-tool-registry GA4GH_TOOL_REGISTRIES\n                        Add a GA4GH tool registry endpoint to use for resolution, default ['https://dockstore.org/api']\n  --on-error {stop,continue}\n                        Desired workflow behavior when a step fails. One of 'stop' (do not submit any more steps) or 'continue' (may submit other\n                        steps that are not downstream from the error). Default is 'stop'.\n  --compute-checksum    Compute checksum of contents while collecting outputs\n  --no-compute-checksum\n                        Do not compute checksum of contents while collecting outputs\n  --relax-path-checks   Relax requirements on path names to permit spaces and hash characters.\n  --force-docker-pull   Pull latest software container image even if it is locally present\n  --no-read-only        Do not set root directory in the container as read-only\n  --overrides OVERRIDES\n                        Read process requirement overrides from file.\n  --target TARGET, -t TARGET\n                        Only execute steps that contribute to listed targets (can be provided more than once).\n  --single-step SINGLE_STEP\n                        Only executes a single step in a workflow. The input object must match that step's inputs. Can be combined with --print-\n                        subgraph.\n  --single-process SINGLE_PROCESS\n                        Only executes the underlying Process (CommandLineTool, ExpressionTool, or sub-Workflow) for the given step in a workflow. This\n                        will not include any step-level processing: 'scatter', 'when'; and there will be no processing of step-level 'default', or\n                        'valueFrom' input modifiers. However, requirements/hints from the step or parent workflow(s) will be inherited as usual.The\n                        input object must match that Process's inputs.\n  --mpi-config-file MPI_CONFIG_FILE\n                        Platform specific configuration for MPI (parallel launcher, its flag etc). See README section 'Running MPI-based tools' for\n                        details of the format.\n  --max-ram MAX_RAM     Maximum amount of RAM to use, e.g 1048576, 512Mi or 2G. Follows k8s resource conventions\n  --max-cores MAX_CORES\n                        Maximum number of CPU cores to use\n  --pod-labels [POD_LABELS]\n                        YAML file of labels to add to Pods submitted\n  --pod-env-vars [POD_ENV_VARS]\n                        YAML file of environment variables to add at runtime to Pods submitted\n  --pod-nodeselectors [POD_NODESELECTORS]\n                        YAML file of node selectors to add to Pods submitted\n  --pod-serviceaccount POD_SERVICEACCOUNT\n                        Service Account to use for pods management\n  --usage-report [USAGE_REPORT]\n                        Output JSON file name to record resource usage\n  --stdout [STDOUT]     Output file name to tee standard output (CWL output object)\n  --stderr [STDERR]     Output file name to tee standard error to (includes tool logs)\n  --tool-logs-basepath [TOOL_LOGS_BASEPATH]\n                        Base path for saving the tool logs\n  --conf CONF           Defines the default values for the CLI arguments\n\nOptions for recording the Docker container identifier into a file.:\n  --cidfile-dir CIDFILE_DIR\n                        Store the Docker container ID into a file in the specified directory.\n  --cidfile-prefix CIDFILE_PREFIX\n                        Specify a prefix to the container ID filename. Final file name will be followed by a timestamp. The default is no prefix.\n\nOptions for recording provenance information of the execution:\n  --provenance PROVENANCE\n                        Save provenance to specified folder as a Research Object that captures and aggregates workflow execution and data products.\n  --enable-user-provenance\n                        Record user account info as part of provenance.\n  --disable-user-provenance\n                        Do not record user account info in provenance.\n  --enable-host-provenance\n                        Record host info as part of provenance.\n  --disable-host-provenance\n                        Do not record host info in provenance.\n  --orcid ORCID         Record user ORCID identifier as part of provenance, e.g. https://orcid.org/0000-0002-1825-0097 or 0000-0002-1825-0097.\n                        Alternatively the environment variable ORCID may be set.\n  --full-name CWL_FULL_NAME\n                        Record full name of user as part of provenance, e.g. Josiah Carberry. You may need to use shell quotes to preserve spaces.\n                        Alternatively the environment variable CWL_FULL_NAME may be set.\n</code></pre>"},{"location":"reference/cwl-commandlinetool/","title":"CWL Command Line Tool Description Standard","text":"<p>The CWL Command Line Tool Description Standard specifies the document schema and execution semantics for wrapping and executing command line tools.</p> <p>Current version: CommandLineTool 1.2.0</p> <p>Previous versions:</p> <ul> <li>CommandLineTool 1.0.2</li> <li>CommandLineTool 1.1.0</li> </ul>"},{"location":"reference/cwl-workflow/","title":"CWL Workflow Description Standard","text":"<p>The CWL Workflow Description Standard specifies the document schema and execution semantics for composing workflows from components such as command line tools and other workflows.</p>"},{"location":"reference/cwltool/","title":"cwltool","text":"<p>The <code>cwltool</code> CLI reference is available at https://cwltool.readthedocs.io/en/latest/cli.html</p>"},{"location":"reference/ogc-ap-bp/","title":"OGC Application Package Best Practice","text":"<p>Published by the Open Geospatial Consortium (OGC) on January 25th 2022, the Best Practice for Earth Observation Application Package  is a guide for developers that wish to package and deploy Earth Observation (EO) Applications for a selected Exploitation Platform.</p> <p>The Best Practice for Earth Observation Application Package is available here</p>"},{"location":"release/ci/","title":"Continuous Integration","text":""},{"location":"release/ci/#application-package-software-configuration-management","title":"Application Package Software Configuration Management","text":"<p>The SCM has the task of tracking and controlling changes in the software as a part of the larger cross-disciplinary field of configuration management. </p> <p>SCM practices include revision control and the establishment of baselines.</p> <p>The Application Package code is hosted on a repository publicly accessible (Github, Bitbucket, a GitLab instance, an institutional software forge, etc.) using one of the version control systems supported by (Subversion, Mercurial and Git)</p> <p>The Application Package code include, at the top level of the source code tree, the following files:</p> <ul> <li>README containing a description of the software (name, purpose, pointers to website, documentation, development platform, contact, and support information, \u2026)</li> <li>AUTHORS, a list of all the persons to be credited for the software.</li> <li>LICENSE, the project license terms. For Open Source Licenses, the standard SPDX license names are used. For large software projects and developers, the REUSE (https://reuse.software/) process and tools can be an option to look at.</li> <li>codemeta.json, a linked data metadata file that helps index the source code in the Software Heritage archive and provides an easy way to link to other related research outputs.</li> </ul> <p>The codemeta.json includes metadata information to support the Continuous Integration phase and it is shown below:</p> codemeta.json<pre><code>{\n    \"@context\": \"https://doi.org/10.5063/schema/codemeta-2.0\",\n    \"@type\": \"SoftwareSourceCode\",\n    \"license\": \"https://spdx.org/licenses/CC-BY-NC-SA-4.0\",\n    \"codeRepository\": \"https://github.com/eoap/mastering-app-package.git\",\n    \"dateCreated\": \"2022-09-01\",\n    \"datePublished\": \"2022-09-25\",\n    \"dateModified\": \"2024-11-07\",\n    \"name\": \"Water Bodies Detection\",\n    \"version\": \"1.1.0\",\n    \"description\": \"The Water Bodies Detection is an Application that uses the NDWI index and the Otsu threshold to detect water bodies using Sentinel-2 or Landsat-9 data\",\n    \"developmentStatus\": \"active\",\n    \"downloadUrl\": \"https://github.com/eoap/mastering-app-package/releases/tag/1.1.0\",\n    \"relatedLink\": [\n        \"https://eoap.github.io/mastering-app-package\"\n    ],\n    \"funder\": {\n        \"@type\": \"Organization\",\n        \"name\": \"Terradue\"\n    },\n    \"keywords\": [\n        \"NDWI\", \"Landsat-9\", \"Sentinel-2\", \"Water Bodies\"\n    ],\n    \"programmingLanguage\": [\n        \"Python\", \"CWL\"\n    ],\n    \"softwareRequirements\": [\n        \"container runtime\",\n        \"cwl runner\"\n    ],\n    \"author\": [\n        {\n            \"@type\": \"Person\",\n            \"givenName\": \"Jane\",\n            \"familyName\": \"Doe\",\n            \"email\": \"jane.doe@acme.earth\",\n            \"affiliation\": {\n                \"@type\": \"Organization\",\n                \"name\": \"ACME\"\n            }\n        },\n        {\n            \"@type\": \"Person\",\n            \"givenName\": \"John\",\n            \"familyName\": \"Doe\",\n            \"email\": \"john.doe@acme.earth\",\n            \"affiliation\": {\n                \"@type\": \"Organization\",\n                \"name\": \"ACME\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"release/ci/#application-package-continuous-integration","title":"Application Package Continuous Integration","text":"<p>A typical Continuous Integration scenario for an Application Package includes the release of the CWL document(s) and publishing the container images to a container registry.</p> <p>This is depicted below: </p> graph TB SCM[(software repository)] SCM -- CWL Workflow --&gt; A SCM -- codemeta.json --&gt; B A(validate CWL Workflow) --&gt; B(extract version) B --&gt; C subgraph Build containers SCM -- Dockerfiles --&gt; C C(build container) --&gt; D(push container)  end D -- push --&gt; CR[(Container Registry)]  D -- container sha256 --&gt; F(\"update Dockerpull/metadata in CWL Workflows\")  F -- push --&gt; AR[(Artifact Registry)] SCM -- codemeta.json --&gt; F <p>Below an example of a GitHub CI configuration implementing the scenario:</p> .github/workflows/build.yaml<pre><code>name: build\non:\n  push:\n    branches:\n    - master\n    - main\n\n    paths:\n    # Only rebuild website when apps have changed\n    - 'water-bodies/**'\n    - .github/**\n    - docs/**\n    - cwl-workflow/*.cwl\n    - codemeta.json\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - uses: actions/setup-python@v2\n      with:\n        python-version: 3.x\n    - run: pip install cwltool\n    - run: cwltool --validate cwl-workflow/app-water-bodies-cloud-native.cwl\n    - run: cwltool --validate cwl-workflow/app-water-body-cloud-native.cwl\n    - run: cwltool --validate cwl-workflow/app-water-body.cwl\n\n  version:\n    needs: validate\n\n    runs-on: ubuntu-latest\n\n    outputs:\n      app-version: ${{ steps.set-version.outputs.version }}\n\n    steps:\n    - uses: actions/checkout@v2\n    - run: echo \"APP_VERSION=$(cat codemeta.json | jq -r .version )\" &gt;&gt; $GITHUB_ENV\n    - run: echo app version is $APP_VERSION\n    - id: set-version\n      run: echo \"::set-output name=version::$APP_VERSION\"\n\n  container-build:\n\n    needs: version\n\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        step: [crop, norm_diff, otsu, stac, stage]\n\n    steps:\n    - uses: actions/checkout@v2\n    - run: echo version ${{needs.version.outputs.app-version}}\n    - run: echo \"${{ secrets.GITHUB_TOKEN }}\" | docker login ghcr.io -u ${{ github.actor }} --password-stdin\n    - name: build &amp; push image\n      run: |\n        IMAGE_ID=ghcr.io/eoap/mastering-app-package/${{ matrix.step }}\n        docker build water-bodies/command-line-tools/${{ matrix.step }} --file water-bodies/command-line-tools/${{ matrix.step }}/Dockerfile --tag ${{ matrix.step }}\n        docker tag ${{ matrix.step }} $IMAGE_ID:${{needs.version.outputs.app-version}}\n        docker push $IMAGE_ID:${{needs.version.outputs.app-version}}\n\n  create-release:\n\n    needs:\n    - container-build\n    - version\n\n    runs-on: ubuntu-latest\n\n    outputs:\n\n      upload_url: ${{ steps.set-upload-url.outputs.upload_url }}\n\n    steps:\n    - name: release\n      uses: actions/create-release@v1\n      id: create_release\n      with:\n        draft: false\n        prerelease: false\n        release_name: ${{needs.version.outputs.app-version}}\n        tag_name: ${{needs.version.outputs.app-version}}\n      env:\n        GITHUB_TOKEN: ${{ github.token }}\n    - id: set-upload-url\n      run: echo \"::set-output name=upload_url::${{ steps.create_release.outputs.upload_url }}\"\n\n  publish-artifacts:\n\n    needs:\n    - create-release\n    - version\n\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        step: [\"app-water-bodies-cloud-native\", \"app-water-body-cloud-native\", \"app-water-body\"]\n\n    steps:\n    - uses: actions/checkout@v2\n    - run: |\n        for step in crop norm_diff otsu stac\n        do\n          tag=\"ghcr.io/eoap/mastering-app-package/${step}:${{needs.version.outputs.app-version}}\"\n          docker pull ${tag}\n          shatag=$( docker inspect ${tag} | yq -r '.[0][\"RepoDigests\"][0]' )\n          for cwl in $(ls cwl-workflow/*.cwl)\n          do\n            s=\"${step}\" t=\"${shatag}\" yq -i eval '(.$graph[] | select (.id == env(s)) ).hints.DockerRequirement.dockerPull = env(t)' $cwl\n          done\n        done\n    - run: |\n        for cwl in $(ls cwl-workflow/*.cwl)\n        do\n          r=$( cat codemeta.json | jq -r \".codeRepository\" ) yq -i eval '.\"s:codeRepository\" = {\"URL\" : env(r)}' $cwl\n          v=\"${{needs.version.outputs.app-version}}\" yq -i eval '.\"s:softwareVersion\" = env(v)' $cwl\n          n=$(cat codemeta.json | jq -r '(.author[0].givenName + \" \" + .author[0].familyName)') \\\n          e=$(cat codemeta.json | jq -r '.author[0].email') \\\n          a=$(cat codemeta.json | jq -r '.author[0].affiliation[\"name\"]') \\\n          yq eval -i  '.\"s:author\" += [{\"class\": \"s:Person\", \"s.name\": env(n), \"s.email\": env(e), \"s.affiliation\": env(a)}]' $cwl\n        done\n    - name: Cleanup downloads folder\n      run: rm -rf downloads/*\n    - run: |\n        mkdir downloads\n        for cwl in \"app-water-bodies-cloud-native\" \"app-water-body-cloud-native\" \"app-water-body\"\n        do\n          cp cwl-workflow/${cwl}.cwl downloads/${cwl}.${{needs.version.outputs.app-version}}.cwl\n        done\n    - uses: actions/upload-artifact@v4\n      with:\n        name: application-package-${{matrix.step}}\n        path: downloads\n        overwrite: true\n    - name: upload linux artifact 1\n      uses: actions/upload-release-asset@v1\n      env:\n        GITHUB_TOKEN: ${{ github.token }}\n      with:\n        upload_url: ${{needs.create-release.outputs.upload_url}}\n        asset_path: downloads/${{matrix.step}}.${{needs.version.outputs.app-version}}.cwl\n        asset_name: ${{matrix.step}}.${{needs.version.outputs.app-version}}.cwl\n        asset_content_type: text/yaml\n</code></pre>"},{"location":"release/scope/","title":"Scope","text":"<p>Releasing an Application Package targets:</p> <ul> <li> <p>building and pushing the container images to a container registry</p> </li> <li> <p>updating the Application Package to reference these container images </p> </li> <li> <p>pushing the updated Application Package to an artifact repository</p> </li> </ul>"},{"location":"scripted/scripted-cloud-native/","title":"TODO","text":""},{"location":"scripted/scripted-k8s-cloud-native/","title":"TODO","text":""}]}